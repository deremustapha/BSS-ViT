{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYdI7X1ZTFW8"
   },
   "source": [
    "# A Vision Transformer without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXaPjKytTFXB"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RVFy9lyMTFXC"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'vega-embed': 'https://cdn.jsdelivr.net/npm/vega-embed@6/build/vega-embed.min', 'vega-lite': 'https://cdn.jsdelivr.net/npm/vega-lite@5/build/vega-lite.min', 'vega': 'https://cdn.jsdelivr.net/npm/vega@5/build/vega.min', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"vega-embed\"], function(vegaEmbed) {\n",
       "\twindow.vegaEmbed = vegaEmbed\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"vega-lite\"], function(vl) {\n",
       "\twindow.vl = vl\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"vega\"], function(vega) {\n",
       "\twindow.vega = vega\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 5;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
       "    }    if (((window['vega'] !== undefined) && (!(window['vega'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/vega@5'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['vegaLite'] !== undefined) && (!(window['vegaLite'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/vega-lite@5'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['vegaEmbed'] !== undefined) && (!(window['vegaEmbed'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/vega-embed@6'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.jsdelivr.net/npm/vega@5\", \"https://cdn.jsdelivr.net/npm/vega-lite@5\", \"https://cdn.jsdelivr.net/npm/vega-embed@6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var css_urls = [\"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.css\", \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/debugger.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/widgets.css\"];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n",
       "    },    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, js_modules, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vega-embed': 'https://cdn.jsdelivr.net/npm/vega-embed@6/build/vega-embed.min', 'vega-lite': 'https://cdn.jsdelivr.net/npm/vega-lite@5/build/vega-lite.min', 'vega': 'https://cdn.jsdelivr.net/npm/vega@5/build/vega.min', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n      require([\"vega-embed\"], function(vegaEmbed) {\n\twindow.vegaEmbed = vegaEmbed\n\ton_load()\n      })\n      require([\"vega-lite\"], function(vl) {\n\twindow.vl = vl\n\ton_load()\n      })\n      require([\"vega\"], function(vega) {\n\twindow.vega = vega\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 5;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['vega'] !== undefined) && (!(window['vega'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/vega@5'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['vegaLite'] !== undefined) && (!(window['vegaLite'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/vega-lite@5'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['vegaEmbed'] !== undefined) && (!(window['vegaEmbed'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/vega-embed@6'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.jsdelivr.net/npm/vega@5\", \"https://cdn.jsdelivr.net/npm/vega-lite@5\", \"https://cdn.jsdelivr.net/npm/vega-embed@6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.css\", \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/debugger.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/widgets.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
       "  font-family: var(--jp-ui-font-size1);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from emgdecompy.decomposition import *\n",
    "from emgdecompy.contrast import *\n",
    "from emgdecompy.viz import *\n",
    "from emgdecompy.preprocessing import *\n",
    "from db1_preprocess_utils import *\n",
    "from feature_extraction import *\n",
    "from experiment_one_utils import *\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.signal import stft\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Setting seed for reproducibiltiy\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfBMfVPwTFXD"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Gux4BYkhTFXE"
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # DATA\n",
    "    batch_size = 36 \n",
    "    buffer_size = batch_size * 2\n",
    "    num_classes = 34\n",
    "    \n",
    "\n",
    "    # ARCHITECTURE\n",
    "    patch_size = 4\n",
    "    projected_dim = 96\n",
    "    num_shift_blocks_per_stages = [2, 4, 8, 2]\n",
    "    epsilon = 1e-5\n",
    "    stochastic_depth_rate = 0.2\n",
    "    mlp_dropout_rate = 0.2\n",
    "    num_div = 12\n",
    "    shift_pixel = 1\n",
    "    mlp_expand_ratio = 2\n",
    "\n",
    "    # OPTIMIZER\n",
    "    lr_start = 1e-5\n",
    "    lr_max = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    # TRAINING\n",
    "    epochs = 150\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8bQMiLSDTFXE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "Loading subject 15\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (17021, 128, 102, 1)\n",
      "The input label shape is (17021,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Training samples: 17021\n",
      "Validation samples: 4256\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\\\AI-Workspace\\\\sEMGClassification\\\\GestureClassificationUsingCViT\\\\data\\\\DB1\\\\raw'\n",
    "\n",
    "signal_type='raw'\n",
    "input_type='raw'\n",
    "n_channels=64\n",
    "low_cut=20\n",
    "high_cut=50\n",
    "order=6 \n",
    "window_length=50\n",
    "overlap=50\n",
    "fs=2048\n",
    "\n",
    "whiten=False\n",
    "center=True\n",
    "extend=True\n",
    "extend_size=1\n",
    "normalize=False\n",
    "mu=0\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "start_subject = 15\n",
    "total_subject = 20\n",
    "session=[1,2]\n",
    "ratio = 0.2\n",
    "\n",
    "noise_db = [5,10,15]\n",
    "std = 1\n",
    "type_of_experiment = 3\n",
    "\n",
    "X_train, y_label, X_test, y_test = get_experiment_data(path, subjects=start_subject, sessions=session,\n",
    "                                                       signal_type='raw', input_type=input_type, \n",
    "                                                       channels=n_channels, low_cut=20, high_cut=500,\n",
    "                                                      order=6, window_size=window_length, overlap=overlap, fs=fs, \n",
    "                                                      extend=extend, center=center, \n",
    "                                                      extend_size=extend_size, whiten=whiten,\n",
    "                                                      normalize=False, mu=0, ratio=ratio)\n",
    "\n",
    "input_size = X_train.shape[0:]\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcNKAkSjTFXG"
   },
   "source": [
    "#### The MLP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wnHOJVXBTFXG"
   },
   "outputs": [],
   "source": [
    "class MLP(layers.Layer):\n",
    "    \"\"\"Get the MLP layer for each shift block.\n",
    "\n",
    "    Args:\n",
    "        mlp_expand_ratio (int): The ratio with which the first feature map is expanded.\n",
    "        mlp_dropout_rate (float): The rate for dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mlp_expand_ratio, mlp_dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp_expand_ratio = mlp_expand_ratio\n",
    "        self.mlp_dropout_rate = mlp_dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_channels = input_shape[-1]\n",
    "        initial_filters = int(self.mlp_expand_ratio * input_channels)\n",
    "\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=initial_filters, activation=tf.nn.gelu,),\n",
    "                layers.Dropout(rate=self.mlp_dropout_rate),\n",
    "                layers.Dense(units=input_channels),\n",
    "                layers.Dropout(rate=self.mlp_dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7uxZ01hTFXH"
   },
   "source": [
    "#### The DropPath layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BV6bd-2kTFXH"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DropPath(layers.Layer):\n",
    "    \"\"\"Drop Path also known as the Stochastic Depth layer.\n",
    "\n",
    "    Refernece:\n",
    "        - https://keras.io/examples/vision/cct/#stochastic-depth-for-regularization\n",
    "        - github.com:rwightman/pytorch-image-models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_path_prob, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_path_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EylUSDgsTFXI"
   },
   "outputs": [],
   "source": [
    "class ShiftViTBlock(layers.Layer):\n",
    "    \"\"\"A unit ShiftViT Block\n",
    "\n",
    "    Args:\n",
    "        shift_pixel (int): The number of pixels to shift. Default to 1.\n",
    "        mlp_expand_ratio (int): The ratio with which MLP features are\n",
    "            expanded. Default to 2.\n",
    "        mlp_dropout_rate (float): The dropout rate used in MLP.\n",
    "        num_div (int): The number of divisions of the feature map's channel.\n",
    "            Totally, 4/num_div of channels will be shifted. Defaults to 12.\n",
    "        epsilon (float): Epsilon constant.\n",
    "        drop_path_prob (float): The drop probability for drop path.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon,\n",
    "        drop_path_prob,\n",
    "        mlp_dropout_rate,\n",
    "        num_div=12,\n",
    "        shift_pixel=1,\n",
    "        mlp_expand_ratio=2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.shift_pixel = shift_pixel\n",
    "        self.mlp_expand_ratio = mlp_expand_ratio\n",
    "        self.mlp_dropout_rate = mlp_dropout_rate\n",
    "        self.num_div = num_div\n",
    "        self.epsilon = epsilon\n",
    "        self.drop_path_prob = drop_path_prob\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.H = input_shape[1]\n",
    "        self.W = input_shape[2]\n",
    "        self.C = input_shape[3]\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n",
    "        self.drop_path = (\n",
    "            DropPath(drop_path_prob=self.drop_path_prob)\n",
    "            if self.drop_path_prob > 0.0\n",
    "            else layers.Activation(\"linear\")\n",
    "        )\n",
    "        self.mlp = MLP(\n",
    "            mlp_expand_ratio=self.mlp_expand_ratio,\n",
    "            mlp_dropout_rate=self.mlp_dropout_rate,\n",
    "        )\n",
    "\n",
    "    def get_shift_pad(self, x, mode):\n",
    "        \"\"\"Shifts the channels according to the mode chosen.\"\"\"\n",
    "        if mode == \"left\":\n",
    "            offset_height = 0\n",
    "            offset_width = 0\n",
    "            target_height = 0\n",
    "            target_width = self.shift_pixel\n",
    "        elif mode == \"right\":\n",
    "            offset_height = 0\n",
    "            offset_width = self.shift_pixel\n",
    "            target_height = 0\n",
    "            target_width = self.shift_pixel\n",
    "        elif mode == \"up\":\n",
    "            offset_height = 0\n",
    "            offset_width = 0\n",
    "            target_height = self.shift_pixel\n",
    "            target_width = 0\n",
    "        else:\n",
    "            offset_height = self.shift_pixel\n",
    "            offset_width = 0\n",
    "            target_height = self.shift_pixel\n",
    "            target_width = 0\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            x,\n",
    "            offset_height=offset_height,\n",
    "            offset_width=offset_width,\n",
    "            target_height=self.H - target_height,\n",
    "            target_width=self.W - target_width,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=offset_height,\n",
    "            offset_width=offset_width,\n",
    "            target_height=self.H,\n",
    "            target_width=self.W,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Split the feature maps\n",
    "        x_splits = tf.split(x, num_or_size_splits=self.C // self.num_div, axis=-1)\n",
    "\n",
    "        # Shift the feature maps\n",
    "        x_splits[0] = self.get_shift_pad(x_splits[0], mode=\"left\")\n",
    "        x_splits[1] = self.get_shift_pad(x_splits[1], mode=\"right\")\n",
    "        x_splits[2] = self.get_shift_pad(x_splits[2], mode=\"up\")\n",
    "        x_splits[3] = self.get_shift_pad(x_splits[3], mode=\"down\")\n",
    "\n",
    "        # Concatenate the shifted and unshifted feature maps\n",
    "        x = tf.concat(x_splits, axis=-1)\n",
    "\n",
    "        # Add the residual connection\n",
    "        shortcut = x\n",
    "        x = shortcut + self.drop_path(self.mlp(self.layer_norm(x)), training=training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P4IM9hZTFXJ"
   },
   "source": [
    "#### The PatchMerging layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0b9kIabiTFXK"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchMerging(layers.Layer):\n",
    "    \"\"\"The Patch Merging layer.\n",
    "\n",
    "    Args:\n",
    "        epsilon (float): The epsilon constant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        filters = 2 * input_shape[-1]\n",
    "        self.reduction = layers.Conv2D(\n",
    "            filters=filters, kernel_size=2, strides=2, padding=\"same\", use_bias=False\n",
    "        )\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply the patch merging algorithm on the feature maps\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17021, 128, 102, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijuL4RoSTFXK"
   },
   "source": [
    "#### Stacked Shift Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l6mYQtrGTFXK"
   },
   "outputs": [],
   "source": [
    "# Note: This layer will have a different depth of stacking\n",
    "# for different stages on the model.\n",
    "class StackedShiftBlocks(layers.Layer):\n",
    "    \"\"\"The layer containing stacked ShiftViTBlocks.\n",
    "\n",
    "    Args:\n",
    "        epsilon (float): The epsilon constant.\n",
    "        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n",
    "        num_shift_blocks (int): The number of shift vit blocks for this stage.\n",
    "        stochastic_depth_rate (float): The maximum drop path rate chosen.\n",
    "        is_merge (boolean): A flag that determines the use of the Patch Merge\n",
    "            layer after the shift vit blocks.\n",
    "        num_div (int): The division of channels of the feature map. Defaults to 12.\n",
    "        shift_pixel (int): The number of pixels to shift. Defaults to 1.\n",
    "        mlp_expand_ratio (int): The ratio with which the initial dense layer of\n",
    "            the MLP is expanded Defaults to 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon,\n",
    "        mlp_dropout_rate,\n",
    "        num_shift_blocks,\n",
    "        stochastic_depth_rate,\n",
    "        is_merge,\n",
    "        num_div=12,\n",
    "        shift_pixel=1,\n",
    "        mlp_expand_ratio=2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.mlp_dropout_rate = mlp_dropout_rate\n",
    "        self.num_shift_blocks = num_shift_blocks\n",
    "        self.stochastic_depth_rate = stochastic_depth_rate\n",
    "        self.is_merge = is_merge\n",
    "        self.num_div = num_div\n",
    "        self.shift_pixel = shift_pixel\n",
    "        self.mlp_expand_ratio = mlp_expand_ratio\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # Calculate stochastic depth probabilities.\n",
    "        # Reference: https://keras.io/examples/vision/cct/#the-final-cct-model\n",
    "        dpr = [\n",
    "            x\n",
    "            for x in np.linspace(\n",
    "                start=0, stop=self.stochastic_depth_rate, num=self.num_shift_blocks\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Build the shift blocks as a list of ShiftViT Blocks\n",
    "        self.shift_blocks = list()\n",
    "        for num in range(self.num_shift_blocks):\n",
    "            self.shift_blocks.append(\n",
    "                ShiftViTBlock(\n",
    "                    num_div=self.num_div,\n",
    "                    epsilon=self.epsilon,\n",
    "                    drop_path_prob=dpr[num],\n",
    "                    mlp_dropout_rate=self.mlp_dropout_rate,\n",
    "                    shift_pixel=self.shift_pixel,\n",
    "                    mlp_expand_ratio=self.mlp_expand_ratio,\n",
    "                )\n",
    "            )\n",
    "        if self.is_merge:\n",
    "            self.patch_merge = PatchMerging(epsilon=self.epsilon)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for shift_block in self.shift_blocks:\n",
    "            x = shift_block(x, training=training)\n",
    "        if self.is_merge:\n",
    "            x = self.patch_merge(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiRqze8_TFXL"
   },
   "source": [
    "## The ShiftViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vCeRmE6zTFXL"
   },
   "outputs": [],
   "source": [
    "class ShiftViTModel(keras.Model):\n",
    "    \"\"\"The ShiftViT Model.\n",
    "\n",
    "    Args:\n",
    "        data_augmentation (keras.Model): A data augmentation model.\n",
    "        projected_dim (int): The dimension to which the patches of the image are\n",
    "            projected.\n",
    "        patch_size (int): The patch size of the images.\n",
    "        num_shift_blocks_per_stages (list[int]): A list of all the number of shit\n",
    "            blocks per stage.\n",
    "        epsilon (float): The epsilon constant.\n",
    "        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n",
    "        stochastic_depth_rate (float): The maximum drop rate probability.\n",
    "        num_div (int): The number of divisions of the channesl of the feature\n",
    "            map. Defaults to 12.\n",
    "        shift_pixel (int): The number of pixel to shift. Default to 1.\n",
    "        mlp_expand_ratio (int): The ratio with which the initial mlp dense layer\n",
    "            is expanded to. Defaults to 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        projected_dim,\n",
    "        patch_size,\n",
    "        num_shift_blocks_per_stages,\n",
    "        epsilon,\n",
    "        mlp_dropout_rate,\n",
    "        stochastic_depth_rate,\n",
    "        num_div=12,\n",
    "        shift_pixel=1,\n",
    "        mlp_expand_ratio=2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_projection = layers.Conv2D(\n",
    "            filters=projected_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.stages = list()\n",
    "        for index, num_shift_blocks in enumerate(num_shift_blocks_per_stages):\n",
    "            if index == len(num_shift_blocks_per_stages) - 1:\n",
    "                # This is the last stage, do not use the patch merge here.\n",
    "                is_merge = False\n",
    "            else:\n",
    "                is_merge = True\n",
    "            # Build the stages.\n",
    "            self.stages.append(\n",
    "                StackedShiftBlocks(\n",
    "                    epsilon=epsilon,\n",
    "                    mlp_dropout_rate=mlp_dropout_rate,\n",
    "                    num_shift_blocks=num_shift_blocks,\n",
    "                    stochastic_depth_rate=stochastic_depth_rate,\n",
    "                    is_merge=is_merge,\n",
    "                    num_div=num_div,\n",
    "                    shift_pixel=shift_pixel,\n",
    "                    mlp_expand_ratio=mlp_expand_ratio,\n",
    "                )\n",
    "            )\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"patch_projection\": self.patch_projection,\n",
    "                \"stages\": self.stages,\n",
    "                \"global_avg_pool\": self.global_avg_pool,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def _calculate_loss(self, data, training=False):\n",
    "        (images, labels) = data\n",
    "\n",
    "        # Create patches and project the pathces.\n",
    "        projected_patches = self.patch_projection(images)\n",
    "\n",
    "        # Pass through the stages\n",
    "        x = projected_patches\n",
    "        for stage in self.stages:\n",
    "            x = stage(x, training=training)\n",
    "\n",
    "        # Get the logits.\n",
    "        logits = self.global_avg_pool(x)\n",
    "\n",
    "        # Calculate the loss and return it.\n",
    "        total_loss = self.compiled_loss(labels, logits)\n",
    "        return total_loss, labels, logits\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, labels, logits = self._calculate_loss(\n",
    "                data=inputs, training=True\n",
    "            )\n",
    "\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.patch_projection.trainable_variables,\n",
    "            self.global_avg_pool.trainable_variables,\n",
    "        ]\n",
    "        train_vars = train_vars + [stage.trainable_variables for stage in self.stages]\n",
    "\n",
    "        # Optimize the gradients.\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        trainable_variable_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                trainable_variable_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(trainable_variable_list)\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        _, labels, logits = self._calculate_loss(data=data, training=False)\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAM-XfFeTFXM"
   },
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8tPoJlzRTFXM"
   },
   "outputs": [],
   "source": [
    "model = ShiftViTModel(\n",
    "    projected_dim=config.projected_dim,\n",
    "    patch_size=config.patch_size,\n",
    "    num_shift_blocks_per_stages=config.num_shift_blocks_per_stages,\n",
    "    epsilon=config.epsilon,\n",
    "    mlp_dropout_rate=config.mlp_dropout_rate,\n",
    "    stochastic_depth_rate=config.stochastic_depth_rate,\n",
    "    num_div=config.num_div,\n",
    "    shift_pixel=config.shift_pixel,\n",
    "    mlp_expand_ratio=config.mlp_expand_ratio,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXJ77G1-TFXM"
   },
   "source": [
    "## Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QYAlXS7CTFXN"
   },
   "outputs": [],
   "source": [
    "# Some code is taken from:\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule that uses a warmup cosine decay schedule.\"\"\"\n",
    "\n",
    "    def __init__(self, lr_start, lr_max, warmup_steps, total_steps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lr_start: The initial learning rate\n",
    "            lr_max: The maximum learning rate to which lr should increase to in\n",
    "                the warmup steps\n",
    "            warmup_steps: The number of steps for which the model warms up\n",
    "            total_steps: The total number of steps for the model training\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr_start = lr_start\n",
    "        self.lr_max = lr_max\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Check whether the total number of steps is larger than the warmup\n",
    "        # steps. If not, then throw a value error.\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\n",
    "                f\"Total number of steps {self.total_steps} must be\"\n",
    "                + f\"larger or equal to warmup steps {self.warmup_steps}.\"\n",
    "            )\n",
    "\n",
    "        # `cos_annealed_lr` is a graph that increases to 1 from the initial\n",
    "        # step to the warmup step. After that this graph decays to -1 at the\n",
    "        # final step mark.\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / tf.cast(self.total_steps - self.warmup_steps, tf.float32)\n",
    "        )\n",
    "\n",
    "        # Shift the mean of the `cos_annealed_lr` graph to 1. Now the grpah goes\n",
    "        # from 0 to 2. Normalize the graph with 0.5 so that now it goes from 0\n",
    "        # to 1. With the normalized graph we scale it with `lr_max` such that\n",
    "        # it goes from 0 to `lr_max`\n",
    "        learning_rate = 0.5 * self.lr_max * (1 + cos_annealed_lr)\n",
    "\n",
    "        # Check whether warmup_steps is more than 0.\n",
    "        if self.warmup_steps > 0:\n",
    "            # Check whether lr_max is larger that lr_start. If not, throw a value\n",
    "            # error.\n",
    "            if self.lr_max < self.lr_start:\n",
    "                raise ValueError(\n",
    "                    f\"lr_start {self.lr_start} must be smaller or\"\n",
    "                    + f\"equal to lr_max {self.lr_max}.\"\n",
    "                )\n",
    "\n",
    "            # Calculate the slope with which the learning rate should increase\n",
    "            # in the warumup schedule. The formula for slope is m = ((b-a)/steps)\n",
    "            slope = (self.lr_max - self.lr_start) / self.warmup_steps\n",
    "\n",
    "            # With the formula for a straight line (y = mx+c) build the warmup\n",
    "            # schedule\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.lr_start\n",
    "\n",
    "            # When the current step is lesser that warmup steps, get the line\n",
    "            # graph. When the current step is greater than the warmup steps, get\n",
    "            # the scaled cos graph.\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "\n",
    "        # When the current step is more that the total steps, return 0 else return\n",
    "        # the calculated graph.\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_steps = int((int(input_size[0]) / n_batches) * n_epochs)\n",
    "\n",
    "# Calculate the number of steps for warmup.\n",
    "warmup_epoch_percentage = 0.15\n",
    "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "\n",
    "# Initialize the warmupcosine schedule.\n",
    "scheduled_lrs = WarmUpCosine(lr_start=lr_start, lr_max=lr_max, warmup_steps=warmup_steps, total_steps=total_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=weight_decay)\n",
    "model.compile(optimizer=optimizer,\n",
    "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
    "\n",
    "history = model.fit(X_train[train], y_train[train], batch_size=n_batches, \n",
    "                            epochs= n_epochs, verbose=1, \n",
    "                            callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD2XZMxmTFXN"
   },
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(input_size, n_folds, n_batches, n_epochs, start_subject, total_subject, session, path, \n",
    "                       input_type,n_channels, extend, extend_size, center, whiten, ratio, noise_db, std,\n",
    "                       dims, patch_size, num_shift_blocks_per_stages,\n",
    "                        epsilon, mlp_dropout_rate, stochastic_depth_rate, num_div, shift_pixel,\n",
    "                        mlp_expand_ratio, type_of_experiment, lr_start, lr_max, weight_decay):\n",
    "   \n",
    "    \n",
    "    if  type_of_experiment == 1:\n",
    "        print('************************************************')\n",
    "        print('Experiment One')\n",
    "        print('************************************************')\n",
    "    elif type_of_experiment == 2:\n",
    "        print('************************************************')\n",
    "        print('Experiment Two')\n",
    "        print('************************************************')\n",
    "    elif type_of_experiment == 3:\n",
    "        print('************************************************')\n",
    "        print('Experiment Three')\n",
    "        print('************************************************')\n",
    "    else:\n",
    "        print('************************************************')\n",
    "        print('Enter Valid Experiment')\n",
    "        print('************************************************')\n",
    "        \n",
    "        \n",
    "    result = pd.DataFrame({\n",
    "    'Subject': [0],\n",
    "    'Validation_accuracy': [0],\n",
    "    'No_noise': [0],\n",
    "    '5_dB': [0],\n",
    "    '10_dB': [0],\n",
    "    '15_dB': [0],\n",
    "    'Fold_1': [0],\n",
    "    'Fold_2': [0],\n",
    "    'Fold_3': [0]\n",
    "    })\n",
    "    \n",
    "            \n",
    "    total_steps = int((int(input_size[0]) / n_batches) * n_epochs)\n",
    "\n",
    "    # Calculate the number of steps for warmup.\n",
    "    warmup_epoch_percentage = 0.15\n",
    "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "\n",
    "    # Initialize the warmupcosine schedule.\n",
    "    scheduled_lrs = WarmUpCosine(lr_start=lr_start, lr_max=lr_max, warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "    \n",
    "    \n",
    "    for s in range(start_subject, (total_subject+1)):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = get_experiment_data(path, subjects=s, sessions=session,\n",
    "                                                       signal_type='raw', input_type=input_type, \n",
    "                                                       channels=n_channels, low_cut=20, high_cut=500,\n",
    "                                                      order=6, window_size=50, overlap=50, fs=2048, \n",
    "                                                      extend=extend,  extend_size=extend_size, \n",
    "                                                      center=center, whiten=whiten,\n",
    "                                                      normalize=False, mu=0, ratio=ratio)\n",
    "\n",
    "    \n",
    "        #X_train, y_train, X_test, y_test = spilt_data(data, label, ratio)\n",
    "        if input_type == 'raw':\n",
    "            print('Adding noise to RAW input test data')\n",
    "            X_test = np.expand_dims(X_test, axis=3)\n",
    "            X_test_1 = add_noise_all_channel(X_test, noise_db[0], std)\n",
    "            X_test_2 = add_noise_all_channel(X_test, noise_db[1], std)\n",
    "            X_test_3 = add_noise_all_channel(X_test, noise_db[2], std)\n",
    "            \n",
    "            print(\"Size of the input test data is {}\".format(X_test_3.shape))\n",
    "      \n",
    "            \n",
    "        elif input_type == 'tkeo':\n",
    "            print('Adding noise to  TKEO input test data')\n",
    "            X_test = tkeo_image(X_test)\n",
    "            X_test = np.expand_dims(X_test, axis=3)\n",
    "            \n",
    "            X_test_1 = add_noise_all_channel(X_test, noise_db[0], std)\n",
    "            X_test_2 = add_noise_all_channel(X_test, noise_db[1], std)\n",
    "            X_test_3 = add_noise_all_channel(X_test, noise_db[2], std)\n",
    "            \n",
    "            \n",
    "            print(\"Size of the input test data is {}\".format(X_test_3.shape))\n",
    "\n",
    "            \n",
    "        elif input_type == 'stft':\n",
    "            print('Adding noise to STFT input test data')\n",
    "            \n",
    "            \n",
    "            X_test_1 = add_noise_all_channel(X_test, noise_db[0], std)\n",
    "            X_test_1 = stft_image(X_test_1, samples=X_test_1.shape[2])\n",
    "            \n",
    "            X_test_2 = add_noise_all_channel(X_test, noise_db[1], std)\n",
    "            X_test_2 = stft_image(X_test_2, samples=X_test_2.shape[2])\n",
    "            \n",
    "            X_test_3 = add_noise_all_channel(X_test, noise_db[2], std)\n",
    "            X_test_3 = stft_image(X_test_3, samples=X_test_3.shape[2])\n",
    "            \n",
    "            X_test = stft_image(X_test, samples=X_test.shape[2])\n",
    "            \n",
    "            \n",
    "            print(\"Size of the input test data is {}\".format(X_test_3.shape))\n",
    "            \n",
    "        else:\n",
    "            print('Use correct input type')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Get the optimizer.\n",
    "        optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=weight_decay)\n",
    "        \n",
    "        kfold = KFold(n_splits=n_folds, shuffle=False)\n",
    "        accuracy_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        fold_no = 1\n",
    "        \n",
    "        convergence_speed = []\n",
    "        \n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=10, mode=\"auto\")\n",
    "        \n",
    "        for train, test in kfold.split(X_train, y_train):\n",
    "        \n",
    "            \n",
    "            \n",
    "            model = ShiftViTModel(\n",
    "            projected_dim=dims,\n",
    "            patch_size=patch_size,\n",
    "            num_shift_blocks_per_stages=num_shift_blocks_per_stages,\n",
    "            epsilon=epsilon,\n",
    "            mlp_dropout_rate=mlp_dropout_rate,\n",
    "            stochastic_depth_rate=stochastic_depth_rate,\n",
    "            num_div=num_div,\n",
    "            shift_pixel=shift_pixel,\n",
    "            mlp_expand_ratio=mlp_expand_ratio,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            model.compile(optimizer=optimizer,\n",
    "                          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
    "            \n",
    "            print('---------------------------------------------------')\n",
    "            print(f'Training for fold {fold_no} -------')\n",
    "            \n",
    "            history = model.fit(X_train[train], y_train[train], batch_size=n_batches, \n",
    "                                epochs= n_epochs, verbose=1, \n",
    "                                callbacks=callback)\n",
    "            \n",
    "            conv_speed = len(history.history['accuracy'])\n",
    "            convergence_speed.append(conv_speed)\n",
    "            \n",
    "            scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "            print(f'Score for fold  {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "            accuracy_per_fold.append(scores[1] *100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "                  \n",
    "            fold_no = fold_no + 1\n",
    "        \n",
    "        print(\"Average Score per fold \")\n",
    "    \n",
    "        for i in range(0, len(accuracy_per_fold)):\n",
    "            print('-----------------------------------------------')\n",
    "            print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {accuracy_per_fold[i]}%')\n",
    "        print('-----------------------------------------------')\n",
    "        print('Average Metrics for all folds: ')\n",
    "        print(f'> Accuracy: {np.mean(accuracy_per_fold)} (+- {np.std(accuracy_per_fold)})')\n",
    "        print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "        print('-----------------------------------------------')\n",
    "        \n",
    "        print('************************************************')\n",
    "        print(f'For subject {s} without noise')\n",
    "        scores_0 = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print('The loss is {} and accuracy is {}%'.format(scores_0[0], (scores_0[1]*100)))\n",
    "        print(f'For subject {s} and noise 5 dB')\n",
    "        scores_1 = model.evaluate(X_test_1, y_test, verbose=0)\n",
    "        print('The loss is {} and accuracy is {}%'.format(scores_1[0], (scores_1[1]*100)))\n",
    "        print(f'For subject {s} and noise 10 dB')\n",
    "        scores_2 = model.evaluate(X_test_2, y_test, verbose=0)\n",
    "        print('The loss is {} and accuracy is {}%'.format(scores_2[0], (scores_2[1]*100)))\n",
    "        print(f'For subject {s} and noise 15 dB')  \n",
    "        scores_3 = model.evaluate(X_test_3, y_test, verbose=0) \n",
    "        print('The loss is {} and accuracy is {}%'.format(scores_3[0], (scores_3[1]*100)))\n",
    "        print('************************************************') \n",
    "\n",
    "        \n",
    "        result.at[s-1, 'Subject'] =  s\n",
    "        result.at[s-1, 'Validation_accuracy'] =  np.mean(accuracy_per_fold)       \n",
    "        result.at[s-1, 'No_noise'] =  scores_0[1]*100             \n",
    "        result.at[s-1, '5_dB'] =  scores_1[1]*100    \n",
    "        result.at[s-1, '10_dB'] =  scores_2[1]*100\n",
    "        result.at[s-1, '15_dB'] =  scores_3[1]*100   \n",
    "        \n",
    "        result.at[s-1, 'Fold_1'] =  convergence_speed[0]\n",
    "        result.at[s-1, 'Fold_2'] =  convergence_speed[1]\n",
    "        result.at[s-1, 'Fold_3'] =  convergence_speed[2] \n",
    "        \n",
    "        save_path = 'without_attention_raw_all.csv'\n",
    "        result.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "Experiment Three\n",
      "************************************************\n",
      "####################################################################################\n",
      "Loading subject 15\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (17021, 128, 102, 1)\n",
      "The input label shape is (17021,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Adding noise to RAW input test data\n",
      "Size of the input test data is (4256, 128, 102, 1)\n",
      "---------------------------------------------------\n",
      "Training for fold 1 -------\n",
      "Epoch 1/150\n",
      "316/316 [==============================] - 71s 152ms/step - loss: 4.2251 - accuracy: 0.0276\n",
      "Epoch 2/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 3.3520 - accuracy: 0.0916\n",
      "Epoch 3/150\n",
      "316/316 [==============================] - 49s 157ms/step - loss: 2.2602 - accuracy: 0.3497\n",
      "Epoch 4/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 1.4487 - accuracy: 0.5819\n",
      "Epoch 5/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 1.0548 - accuracy: 0.6975\n",
      "Epoch 6/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.8219 - accuracy: 0.7614\n",
      "Epoch 7/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.6583 - accuracy: 0.8110\n",
      "Epoch 8/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.5632 - accuracy: 0.8348\n",
      "Epoch 9/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.4801 - accuracy: 0.8609\n",
      "Epoch 10/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.4208 - accuracy: 0.8739\n",
      "Epoch 11/150\n",
      "316/316 [==============================] - 49s 153ms/step - loss: 0.3788 - accuracy: 0.8868\n",
      "Epoch 12/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.3463 - accuracy: 0.8949\n",
      "Epoch 13/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.3062 - accuracy: 0.9116\n",
      "Epoch 14/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.2932 - accuracy: 0.9130\n",
      "Epoch 15/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.2957 - accuracy: 0.9083\n",
      "Epoch 16/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.2520 - accuracy: 0.9224\n",
      "Epoch 17/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.2147 - accuracy: 0.9345\n",
      "Epoch 18/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.2332 - accuracy: 0.9270\n",
      "Epoch 19/150\n",
      "316/316 [==============================] - 52s 166ms/step - loss: 0.2122 - accuracy: 0.9363\n",
      "Epoch 20/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.1885 - accuracy: 0.9443\n",
      "Epoch 21/150\n",
      "316/316 [==============================] - 47s 148ms/step - loss: 0.1794 - accuracy: 0.9468\n",
      "Epoch 22/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.2000 - accuracy: 0.9378\n",
      "Epoch 23/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.1752 - accuracy: 0.9449\n",
      "Epoch 24/150\n",
      "316/316 [==============================] - 46s 147ms/step - loss: 0.1629 - accuracy: 0.9506\n",
      "Epoch 25/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.1460 - accuracy: 0.9547\n",
      "Epoch 26/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1648 - accuracy: 0.9500\n",
      "Epoch 27/150\n",
      "316/316 [==============================] - 47s 150ms/step - loss: 0.1370 - accuracy: 0.9590\n",
      "Epoch 28/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1507 - accuracy: 0.9536\n",
      "Epoch 29/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.1290 - accuracy: 0.9587\n",
      "Epoch 30/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.1402 - accuracy: 0.9551\n",
      "Epoch 31/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1113 - accuracy: 0.9666\n",
      "Epoch 32/150\n",
      "316/316 [==============================] - 47s 150ms/step - loss: 0.1220 - accuracy: 0.9611\n",
      "Epoch 33/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1631 - accuracy: 0.9497\n",
      "Epoch 34/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.1277 - accuracy: 0.9598\n",
      "Epoch 35/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0862 - accuracy: 0.9725\n",
      "Epoch 36/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0825 - accuracy: 0.9755\n",
      "Epoch 37/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0752 - accuracy: 0.9774\n",
      "Epoch 38/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0794 - accuracy: 0.9736\n",
      "Epoch 39/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0924 - accuracy: 0.9717\n",
      "Epoch 40/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0839 - accuracy: 0.9729\n",
      "Epoch 41/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0812 - accuracy: 0.9749\n",
      "Epoch 42/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0759 - accuracy: 0.9767\n",
      "Epoch 43/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0523 - accuracy: 0.9827\n",
      "Epoch 44/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0549 - accuracy: 0.9827\n",
      "Epoch 45/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0592 - accuracy: 0.9814\n",
      "Epoch 46/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0822 - accuracy: 0.9727\n",
      "Epoch 47/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0607 - accuracy: 0.9818\n",
      "Epoch 48/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0572 - accuracy: 0.9829\n",
      "Epoch 49/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0693 - accuracy: 0.9783\n",
      "Epoch 50/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0548 - accuracy: 0.9820\n",
      "Epoch 51/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0528 - accuracy: 0.9842\n",
      "Epoch 52/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0420 - accuracy: 0.9878\n",
      "Epoch 53/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0483 - accuracy: 0.9862\n",
      "Epoch 54/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0682 - accuracy: 0.9790\n",
      "Epoch 55/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0482 - accuracy: 0.9848\n",
      "Epoch 56/150\n",
      "316/316 [==============================] - 56s 179ms/step - loss: 0.0492 - accuracy: 0.9853\n",
      "Epoch 57/150\n",
      "316/316 [==============================] - 55s 174ms/step - loss: 0.0427 - accuracy: 0.9875\n",
      "Epoch 58/150\n",
      "316/316 [==============================] - 53s 167ms/step - loss: 0.0495 - accuracy: 0.9853\n",
      "Epoch 59/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0429 - accuracy: 0.9882\n",
      "Epoch 60/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0444 - accuracy: 0.9870\n",
      "Epoch 61/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0370 - accuracy: 0.9898\n",
      "Epoch 62/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0507 - accuracy: 0.9845\n",
      "Epoch 63/150\n",
      "316/316 [==============================] - 53s 166ms/step - loss: 0.0358 - accuracy: 0.9899\n",
      "Epoch 64/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0190 - accuracy: 0.9952\n",
      "Epoch 65/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0510 - accuracy: 0.9853\n",
      "Epoch 66/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0536 - accuracy: 0.9847\n",
      "Epoch 67/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0461 - accuracy: 0.9873\n",
      "Epoch 68/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0282 - accuracy: 0.9915\n",
      "Epoch 69/150\n",
      "316/316 [==============================] - 51s 162ms/step - loss: 0.0299 - accuracy: 0.9921\n",
      "Epoch 70/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0457 - accuracy: 0.9861\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 52s 163ms/step - loss: 0.0546 - accuracy: 0.9841\n",
      "Epoch 72/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0307 - accuracy: 0.9907\n",
      "Epoch 73/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0243 - accuracy: 0.9932\n",
      "Epoch 74/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0249 - accuracy: 0.9935\n",
      "Score for fold  1: loss of 0.09781122952699661; accuracy of 97.25061655044556%\n",
      "---------------------------------------------------\n",
      "Training for fold 2 -------\n",
      "Epoch 1/150\n",
      "316/316 [==============================] - 72s 159ms/step - loss: 4.7021 - accuracy: 0.0311\n",
      "Epoch 2/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 3.6357 - accuracy: 0.0292\n",
      "Epoch 3/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 3.5913 - accuracy: 0.0330\n",
      "Epoch 4/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 3.5808 - accuracy: 0.0265\n",
      "Epoch 5/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 3.5696 - accuracy: 0.0316\n",
      "Epoch 6/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 3.5607 - accuracy: 0.0298\n",
      "Epoch 7/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 3.5592 - accuracy: 0.0287\n",
      "Epoch 8/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 3.5517 - accuracy: 0.0305\n",
      "Epoch 9/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 3.5511 - accuracy: 0.0295\n",
      "Epoch 10/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 3.5456 - accuracy: 0.0298\n",
      "Epoch 11/150\n",
      "316/316 [==============================] - 52s 164ms/step - loss: 3.2785 - accuracy: 0.0773\n",
      "Epoch 12/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 2.4433 - accuracy: 0.2823\n",
      "Epoch 13/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 1.7668 - accuracy: 0.4688\n",
      "Epoch 14/150\n",
      "316/316 [==============================] - 47s 150ms/step - loss: 1.2777 - accuracy: 0.6180\n",
      "Epoch 15/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.9670 - accuracy: 0.7115\n",
      "Epoch 16/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.7813 - accuracy: 0.7647\n",
      "Epoch 17/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.6677 - accuracy: 0.7983\n",
      "Epoch 18/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.5749 - accuracy: 0.8281\n",
      "Epoch 19/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.5038 - accuracy: 0.8473\n",
      "Epoch 20/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.4295 - accuracy: 0.8685\n",
      "Epoch 21/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.3921 - accuracy: 0.8801\n",
      "Epoch 22/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.3422 - accuracy: 0.8957\n",
      "Epoch 23/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.2983 - accuracy: 0.9107\n",
      "Epoch 24/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.2729 - accuracy: 0.9182\n",
      "Epoch 25/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.2525 - accuracy: 0.9219\n",
      "Epoch 26/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.2032 - accuracy: 0.9401\n",
      "Epoch 27/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.1914 - accuracy: 0.9445\n",
      "Epoch 28/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.1722 - accuracy: 0.9509\n",
      "Epoch 29/150\n",
      "316/316 [==============================] - 50s 160ms/step - loss: 0.1513 - accuracy: 0.9555\n",
      "Epoch 30/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.1509 - accuracy: 0.9560\n",
      "Epoch 31/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.1208 - accuracy: 0.9653\n",
      "Epoch 32/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.1226 - accuracy: 0.9632\n",
      "Epoch 33/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0965 - accuracy: 0.9740\n",
      "Epoch 34/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 0.1040 - accuracy: 0.9700\n",
      "Epoch 35/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0928 - accuracy: 0.9742\n",
      "Epoch 36/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.1033 - accuracy: 0.9692\n",
      "Epoch 37/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0887 - accuracy: 0.9758\n",
      "Epoch 38/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0649 - accuracy: 0.9848\n",
      "Epoch 39/150\n",
      "316/316 [==============================] - 50s 160ms/step - loss: 0.0691 - accuracy: 0.9819\n",
      "Epoch 40/150\n",
      "316/316 [==============================] - 52s 166ms/step - loss: 0.0806 - accuracy: 0.9767\n",
      "Epoch 41/150\n",
      "316/316 [==============================] - 52s 166ms/step - loss: 0.0675 - accuracy: 0.9820\n",
      "Epoch 42/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0705 - accuracy: 0.9796\n",
      "Epoch 43/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0507 - accuracy: 0.9883\n",
      "Epoch 44/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0698 - accuracy: 0.9811\n",
      "Epoch 45/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0574 - accuracy: 0.9843\n",
      "Epoch 46/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0478 - accuracy: 0.9892\n",
      "Epoch 47/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0461 - accuracy: 0.9895\n",
      "Epoch 48/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0529 - accuracy: 0.9863\n",
      "Epoch 49/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0591 - accuracy: 0.9842\n",
      "Epoch 50/150\n",
      "316/316 [==============================] - 49s 157ms/step - loss: 0.0594 - accuracy: 0.9840\n",
      "Epoch 51/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0373 - accuracy: 0.9920\n",
      "Epoch 52/150\n",
      "316/316 [==============================] - 52s 165ms/step - loss: 0.0329 - accuracy: 0.9924\n",
      "Epoch 53/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0652 - accuracy: 0.9818\n",
      "Epoch 54/150\n",
      "316/316 [==============================] - 55s 173ms/step - loss: 0.0395 - accuracy: 0.9916\n",
      "Epoch 55/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0473 - accuracy: 0.9889\n",
      "Epoch 56/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0301 - accuracy: 0.9940\n",
      "Epoch 57/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0526 - accuracy: 0.9870\n",
      "Epoch 58/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0356 - accuracy: 0.9929\n",
      "Epoch 59/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0373 - accuracy: 0.9910\n",
      "Epoch 60/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0462 - accuracy: 0.9890\n",
      "Epoch 61/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0383 - accuracy: 0.9916\n",
      "Epoch 62/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0351 - accuracy: 0.9923\n",
      "Epoch 63/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0364 - accuracy: 0.9910\n",
      "Epoch 64/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 0.0509 - accuracy: 0.9881\n",
      "Epoch 65/150\n",
      "316/316 [==============================] - 53s 169ms/step - loss: 0.0237 - accuracy: 0.9964\n",
      "Epoch 66/150\n",
      "316/316 [==============================] - 51s 162ms/step - loss: 0.0369 - accuracy: 0.9914\n",
      "Epoch 67/150\n",
      "316/316 [==============================] - 50s 160ms/step - loss: 0.0366 - accuracy: 0.9919\n",
      "Epoch 68/150\n",
      "316/316 [==============================] - 49s 153ms/step - loss: 0.0323 - accuracy: 0.9929\n",
      "Epoch 69/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0549 - accuracy: 0.9856\n",
      "Epoch 70/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0384 - accuracy: 0.9909\n",
      "Epoch 71/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0356 - accuracy: 0.9925\n",
      "Epoch 72/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0348 - accuracy: 0.9925\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 48s 150ms/step - loss: 0.0300 - accuracy: 0.9944\n",
      "Epoch 74/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0343 - accuracy: 0.9919\n",
      "Epoch 75/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0549 - accuracy: 0.9884\n",
      "Score for fold  2: loss of 0.05890681594610214; accuracy of 98.06133508682251%\n",
      "---------------------------------------------------\n",
      "Training for fold 3 -------\n",
      "Epoch 1/150\n",
      "316/316 [==============================] - 67s 150ms/step - loss: 3.9020 - accuracy: 0.0444\n",
      "Epoch 2/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 3.1542 - accuracy: 0.0967\n",
      "Epoch 3/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 2.5156 - accuracy: 0.2528\n",
      "Epoch 4/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 1.7488 - accuracy: 0.4743\n",
      "Epoch 5/150\n",
      "316/316 [==============================] - 48s 150ms/step - loss: 1.1675 - accuracy: 0.6624\n",
      "Epoch 6/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.8112 - accuracy: 0.7659\n",
      "Epoch 7/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.6246 - accuracy: 0.8190\n",
      "Epoch 8/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.4985 - accuracy: 0.8559\n",
      "Epoch 9/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.4159 - accuracy: 0.8809\n",
      "Epoch 10/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.3422 - accuracy: 0.9040\n",
      "Epoch 11/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.2939 - accuracy: 0.9181\n",
      "Epoch 12/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.2417 - accuracy: 0.9336\n",
      "Epoch 13/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.2267 - accuracy: 0.9379\n",
      "Epoch 14/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1984 - accuracy: 0.9479\n",
      "Epoch 15/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1782 - accuracy: 0.9523\n",
      "Epoch 16/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1706 - accuracy: 0.9562\n",
      "Epoch 17/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.1489 - accuracy: 0.9597\n",
      "Epoch 18/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1327 - accuracy: 0.9670\n",
      "Epoch 19/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1259 - accuracy: 0.9710\n",
      "Epoch 20/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1157 - accuracy: 0.9721\n",
      "Epoch 21/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1211 - accuracy: 0.9677\n",
      "Epoch 22/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1023 - accuracy: 0.9770\n",
      "Epoch 23/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0987 - accuracy: 0.9768\n",
      "Epoch 24/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1072 - accuracy: 0.9743\n",
      "Epoch 25/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0975 - accuracy: 0.9773\n",
      "Epoch 26/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0837 - accuracy: 0.9841\n",
      "Epoch 27/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0867 - accuracy: 0.9817\n",
      "Epoch 28/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0831 - accuracy: 0.9833\n",
      "Epoch 29/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0811 - accuracy: 0.9839\n",
      "Epoch 30/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0907 - accuracy: 0.9799\n",
      "Epoch 31/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0760 - accuracy: 0.9854\n",
      "Epoch 32/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0837 - accuracy: 0.9824\n",
      "Epoch 33/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0820 - accuracy: 0.9838\n",
      "Epoch 34/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0799 - accuracy: 0.9851\n",
      "Epoch 35/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0869 - accuracy: 0.9826\n",
      "Epoch 36/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0812 - accuracy: 0.9851\n",
      "Epoch 37/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0831 - accuracy: 0.9859\n",
      "Epoch 38/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0876 - accuracy: 0.9845\n",
      "Epoch 39/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0816 - accuracy: 0.9876\n",
      "Epoch 40/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0817 - accuracy: 0.9861\n",
      "Epoch 41/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0885 - accuracy: 0.9841\n",
      "Epoch 42/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0857 - accuracy: 0.9861\n",
      "Epoch 43/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1007 - accuracy: 0.9809\n",
      "Epoch 44/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0917 - accuracy: 0.9841\n",
      "Epoch 45/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1016 - accuracy: 0.9836\n",
      "Epoch 46/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1042 - accuracy: 0.9836\n",
      "Epoch 47/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1029 - accuracy: 0.9865\n",
      "Epoch 48/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1111 - accuracy: 0.9833\n",
      "Epoch 49/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1147 - accuracy: 0.9808\n",
      "Score for fold  3: loss of 0.0958564504981041; accuracy of 97.46165871620178%\n",
      "Average Score per fold \n",
      "-----------------------------------------------\n",
      "> Fold 1 - Loss: 0.09781122952699661 - Accuracy: 97.25061655044556%\n",
      "-----------------------------------------------\n",
      "> Fold 2 - Loss: 0.05890681594610214 - Accuracy: 98.06133508682251%\n",
      "-----------------------------------------------\n",
      "> Fold 3 - Loss: 0.0958564504981041 - Accuracy: 97.46165871620178%\n",
      "-----------------------------------------------\n",
      "Average Metrics for all folds: \n",
      "> Accuracy: 97.59120345115662 (+- 0.3434166711854914)\n",
      "> Loss: 0.08419149865706761\n",
      "-----------------------------------------------\n",
      "************************************************\n",
      "For subject 15 without noise\n",
      "The loss is 0.10636607557535172 and accuracy is 97.18044996261597%\n",
      "For subject 15 and noise 5 dB\n",
      "The loss is 0.21813294291496277 and accuracy is 96.029132604599%\n",
      "For subject 15 and noise 10 dB\n",
      "The loss is 0.21404090523719788 and accuracy is 95.9116518497467%\n",
      "For subject 15 and noise 15 dB\n",
      "The loss is 0.20632266998291016 and accuracy is 95.77067494392395%\n",
      "************************************************\n",
      "####################################################################################\n",
      "Loading subject 16\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (16894, 128, 102, 1)\n",
      "The input label shape is (16894,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Adding noise to RAW input test data\n",
      "Size of the input test data is (4224, 128, 102, 1)\n",
      "---------------------------------------------------\n",
      "Training for fold 1 -------\n",
      "Epoch 1/150\n",
      "313/313 [==============================] - 67s 150ms/step - loss: 4.2307 - accuracy: 0.0276\n",
      "Epoch 2/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 3.5643 - accuracy: 0.0625\n",
      "Epoch 3/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 2.6557 - accuracy: 0.2314\n",
      "Epoch 4/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 1.9219 - accuracy: 0.4405\n",
      "Epoch 5/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 1.4811 - accuracy: 0.5741\n",
      "Epoch 6/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 1.1427 - accuracy: 0.6760\n",
      "Epoch 7/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.9044 - accuracy: 0.7447\n",
      "Epoch 8/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 47s 151ms/step - loss: 0.7397 - accuracy: 0.7933\n",
      "Epoch 9/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.6213 - accuracy: 0.8264\n",
      "Epoch 10/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.5291 - accuracy: 0.8493\n",
      "Epoch 11/150\n",
      "313/313 [==============================] - 47s 150ms/step - loss: 0.4680 - accuracy: 0.8651\n",
      "Epoch 12/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.4007 - accuracy: 0.8836\n",
      "Epoch 13/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.3740 - accuracy: 0.8912\n",
      "Epoch 14/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.3533 - accuracy: 0.8945\n",
      "Epoch 15/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.3057 - accuracy: 0.9135\n",
      "Epoch 16/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.2837 - accuracy: 0.9164\n",
      "Epoch 17/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2669 - accuracy: 0.9216\n",
      "Epoch 18/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2496 - accuracy: 0.9248\n",
      "Epoch 19/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2212 - accuracy: 0.9348\n",
      "Epoch 20/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2348 - accuracy: 0.9278\n",
      "Epoch 21/150\n",
      "313/313 [==============================] - 47s 150ms/step - loss: 0.1977 - accuracy: 0.9380\n",
      "Epoch 22/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.2036 - accuracy: 0.9364\n",
      "Epoch 23/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1938 - accuracy: 0.9416\n",
      "Epoch 24/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1756 - accuracy: 0.9480\n",
      "Epoch 25/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1698 - accuracy: 0.9492\n",
      "Epoch 26/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1487 - accuracy: 0.9550\n",
      "Epoch 27/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1527 - accuracy: 0.9531\n",
      "Epoch 28/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.1640 - accuracy: 0.9503\n",
      "Epoch 29/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1488 - accuracy: 0.9535\n",
      "Epoch 30/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1270 - accuracy: 0.9599\n",
      "Epoch 31/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1186 - accuracy: 0.9654\n",
      "Epoch 32/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1632 - accuracy: 0.9503\n",
      "Epoch 33/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1273 - accuracy: 0.9606\n",
      "Epoch 34/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1401 - accuracy: 0.9571\n",
      "Epoch 35/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0819 - accuracy: 0.9741\n",
      "Epoch 36/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1023 - accuracy: 0.9695\n",
      "Epoch 37/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1003 - accuracy: 0.9692\n",
      "Epoch 38/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.1020 - accuracy: 0.9699\n",
      "Epoch 39/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1010 - accuracy: 0.9689\n",
      "Epoch 40/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0882 - accuracy: 0.9747\n",
      "Epoch 41/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0784 - accuracy: 0.9761\n",
      "Epoch 42/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0747 - accuracy: 0.9779\n",
      "Epoch 43/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0662 - accuracy: 0.9800\n",
      "Epoch 44/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0945 - accuracy: 0.9735\n",
      "Epoch 45/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0631 - accuracy: 0.9798\n",
      "Epoch 46/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0892 - accuracy: 0.9723\n",
      "Epoch 47/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0558 - accuracy: 0.9843\n",
      "Epoch 48/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0502 - accuracy: 0.9865\n",
      "Epoch 49/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0531 - accuracy: 0.9854\n",
      "Epoch 50/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0823 - accuracy: 0.9750\n",
      "Epoch 51/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0687 - accuracy: 0.9798\n",
      "Epoch 52/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0572 - accuracy: 0.9830\n",
      "Epoch 53/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0515 - accuracy: 0.9849\n",
      "Epoch 54/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0564 - accuracy: 0.9836\n",
      "Epoch 55/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0635 - accuracy: 0.9806\n",
      "Epoch 56/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0378 - accuracy: 0.9897\n",
      "Epoch 57/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0720 - accuracy: 0.9791\n",
      "Epoch 58/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0618 - accuracy: 0.9830\n",
      "Epoch 59/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0359 - accuracy: 0.9908\n",
      "Epoch 60/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0519 - accuracy: 0.9846\n",
      "Epoch 61/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0566 - accuracy: 0.9824\n",
      "Epoch 62/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0477 - accuracy: 0.9866\n",
      "Epoch 63/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0505 - accuracy: 0.9853\n",
      "Epoch 64/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0226 - accuracy: 0.9945\n",
      "Epoch 65/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0351 - accuracy: 0.9903\n",
      "Epoch 66/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0676 - accuracy: 0.9808\n",
      "Epoch 67/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0460 - accuracy: 0.9869\n",
      "Epoch 68/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0509 - accuracy: 0.9861\n",
      "Epoch 69/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0438 - accuracy: 0.9873\n",
      "Epoch 70/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0371 - accuracy: 0.9904\n",
      "Epoch 71/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0496 - accuracy: 0.9838\n",
      "Epoch 72/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0309 - accuracy: 0.9907\n",
      "Epoch 73/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0285 - accuracy: 0.9924\n",
      "Epoch 74/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0450 - accuracy: 0.9871\n",
      "Score for fold  1: loss of 0.10807187855243683; accuracy of 97.49644994735718%\n",
      "---------------------------------------------------\n",
      "Training for fold 2 -------\n",
      "Epoch 1/150\n",
      "313/313 [==============================] - 67s 151ms/step - loss: 4.6153 - accuracy: 0.0266\n",
      "Epoch 2/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 3.6160 - accuracy: 0.0288\n",
      "Epoch 3/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 3.5868 - accuracy: 0.0282\n",
      "Epoch 4/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 3.5695 - accuracy: 0.0267\n",
      "Epoch 5/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 3.5610 - accuracy: 0.0313\n",
      "Epoch 6/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 3.5559 - accuracy: 0.0279\n",
      "Epoch 7/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 3.5495 - accuracy: 0.0307\n",
      "Epoch 8/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 3.5481 - accuracy: 0.0290\n",
      "Epoch 9/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 3.5453 - accuracy: 0.0296\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 48s 152ms/step - loss: 3.5461 - accuracy: 0.0264\n",
      "Epoch 11/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 3.5399 - accuracy: 0.0312\n",
      "Epoch 12/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 3.5014 - accuracy: 0.0426\n",
      "Epoch 13/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 3.3260 - accuracy: 0.0805\n",
      "Epoch 14/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 2.8099 - accuracy: 0.1819\n",
      "Epoch 15/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 2.4579 - accuracy: 0.2837\n",
      "Epoch 16/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 2.1692 - accuracy: 0.3593\n",
      "Epoch 17/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 1.9741 - accuracy: 0.4198\n",
      "Epoch 18/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 1.6666 - accuracy: 0.5118\n",
      "Epoch 19/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 1.3734 - accuracy: 0.6078\n",
      "Epoch 20/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 1.1260 - accuracy: 0.6761\n",
      "Epoch 21/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.9610 - accuracy: 0.7292\n",
      "Epoch 22/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.8224 - accuracy: 0.7676\n",
      "Epoch 23/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.7059 - accuracy: 0.7955\n",
      "Epoch 24/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.6062 - accuracy: 0.8233\n",
      "Epoch 25/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.5107 - accuracy: 0.8526\n",
      "Epoch 26/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.4443 - accuracy: 0.8686\n",
      "Epoch 27/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.3917 - accuracy: 0.8857\n",
      "Epoch 28/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.3189 - accuracy: 0.9063\n",
      "Epoch 29/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2866 - accuracy: 0.9146\n",
      "Epoch 30/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.2538 - accuracy: 0.9213\n",
      "Epoch 31/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2262 - accuracy: 0.9334\n",
      "Epoch 32/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.2018 - accuracy: 0.9424\n",
      "Epoch 33/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1797 - accuracy: 0.9466\n",
      "Epoch 34/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1677 - accuracy: 0.9505\n",
      "Epoch 35/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1503 - accuracy: 0.9560\n",
      "Epoch 36/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.1424 - accuracy: 0.9595\n",
      "Epoch 37/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.1356 - accuracy: 0.9622\n",
      "Epoch 38/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1236 - accuracy: 0.9645\n",
      "Epoch 39/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1088 - accuracy: 0.9712\n",
      "Epoch 40/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.1017 - accuracy: 0.9714\n",
      "Epoch 41/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0919 - accuracy: 0.9754\n",
      "Epoch 42/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0996 - accuracy: 0.9742\n",
      "Epoch 43/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0919 - accuracy: 0.9756\n",
      "Epoch 44/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0891 - accuracy: 0.9778\n",
      "Epoch 45/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0900 - accuracy: 0.9762\n",
      "Epoch 46/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0815 - accuracy: 0.9774\n",
      "Epoch 47/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0777 - accuracy: 0.9806\n",
      "Epoch 48/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0652 - accuracy: 0.9832\n",
      "Epoch 49/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0759 - accuracy: 0.9798\n",
      "Epoch 50/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0639 - accuracy: 0.9835\n",
      "Epoch 51/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0689 - accuracy: 0.9824\n",
      "Epoch 52/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0631 - accuracy: 0.9838\n",
      "Epoch 53/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0473 - accuracy: 0.9895\n",
      "Epoch 54/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0681 - accuracy: 0.9839\n",
      "Epoch 55/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0752 - accuracy: 0.9813\n",
      "Epoch 56/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0534 - accuracy: 0.9870\n",
      "Epoch 57/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0671 - accuracy: 0.9839\n",
      "Epoch 58/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0547 - accuracy: 0.9860\n",
      "Epoch 59/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0550 - accuracy: 0.9872\n",
      "Epoch 60/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0546 - accuracy: 0.9873\n",
      "Epoch 61/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0498 - accuracy: 0.9893\n",
      "Epoch 62/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0589 - accuracy: 0.9867\n",
      "Epoch 63/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0495 - accuracy: 0.9884\n",
      "Score for fold  2: loss of 0.10758189111948013; accuracy of 97.14082479476929%\n",
      "---------------------------------------------------\n",
      "Training for fold 3 -------\n",
      "Epoch 1/150\n",
      "313/313 [==============================] - 66s 151ms/step - loss: 4.0176 - accuracy: 0.0463\n",
      "Epoch 2/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 3.2193 - accuracy: 0.0981\n",
      "Epoch 3/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 2.5562 - accuracy: 0.2487\n",
      "Epoch 4/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 1.9053 - accuracy: 0.4337\n",
      "Epoch 5/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 1.4245 - accuracy: 0.5786\n",
      "Epoch 6/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 1.0699 - accuracy: 0.6859\n",
      "Epoch 7/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.8206 - accuracy: 0.7587\n",
      "Epoch 8/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.6663 - accuracy: 0.8030\n",
      "Epoch 9/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.5258 - accuracy: 0.8481\n",
      "Epoch 10/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.4470 - accuracy: 0.8701\n",
      "Epoch 11/150\n",
      "313/313 [==============================] - 48s 154ms/step - loss: 0.3708 - accuracy: 0.8939\n",
      "Epoch 12/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.3164 - accuracy: 0.9133\n",
      "Epoch 13/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.2734 - accuracy: 0.9272\n",
      "Epoch 14/150\n",
      "313/313 [==============================] - 48s 155ms/step - loss: 0.2441 - accuracy: 0.9331\n",
      "Epoch 15/150\n",
      "313/313 [==============================] - 49s 155ms/step - loss: 0.2160 - accuracy: 0.9417\n",
      "Epoch 16/150\n",
      "313/313 [==============================] - 50s 160ms/step - loss: 0.1743 - accuracy: 0.9558\n",
      "Epoch 17/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.1652 - accuracy: 0.9578\n",
      "Epoch 18/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.1475 - accuracy: 0.9637\n",
      "Epoch 19/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.1385 - accuracy: 0.9632\n",
      "Epoch 20/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1156 - accuracy: 0.9720\n",
      "Epoch 21/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1174 - accuracy: 0.9725\n",
      "Epoch 22/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.1016 - accuracy: 0.9768\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0935 - accuracy: 0.9786\n",
      "Epoch 24/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0994 - accuracy: 0.9773\n",
      "Epoch 25/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0868 - accuracy: 0.9817\n",
      "Epoch 26/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0801 - accuracy: 0.9822\n",
      "Epoch 27/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0685 - accuracy: 0.9855\n",
      "Epoch 28/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0734 - accuracy: 0.9852\n",
      "Epoch 29/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0734 - accuracy: 0.9838\n",
      "Epoch 30/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0770 - accuracy: 0.9830\n",
      "Epoch 31/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0684 - accuracy: 0.9862\n",
      "Epoch 32/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0765 - accuracy: 0.9820\n",
      "Epoch 33/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0735 - accuracy: 0.9859\n",
      "Epoch 34/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0593 - accuracy: 0.9877\n",
      "Epoch 35/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0582 - accuracy: 0.9893\n",
      "Epoch 36/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0587 - accuracy: 0.9891\n",
      "Epoch 37/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0691 - accuracy: 0.9865\n",
      "Epoch 38/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0568 - accuracy: 0.9917\n",
      "Epoch 39/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0773 - accuracy: 0.9836\n",
      "Epoch 40/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0622 - accuracy: 0.9874\n",
      "Epoch 41/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0623 - accuracy: 0.9888\n",
      "Epoch 42/150\n",
      "313/313 [==============================] - 47s 151ms/step - loss: 0.0609 - accuracy: 0.9901\n",
      "Epoch 43/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0625 - accuracy: 0.9893\n",
      "Epoch 44/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0676 - accuracy: 0.9869\n",
      "Epoch 45/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0693 - accuracy: 0.9874\n",
      "Epoch 46/150\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0628 - accuracy: 0.9909\n",
      "Epoch 47/150\n",
      "313/313 [==============================] - 47s 152ms/step - loss: 0.0644 - accuracy: 0.9893\n",
      "Epoch 48/150\n",
      "313/313 [==============================] - 48s 153ms/step - loss: 0.0725 - accuracy: 0.9867\n",
      "Score for fold  3: loss of 0.0895446315407753; accuracy of 97.69135117530823%\n",
      "Average Score per fold \n",
      "-----------------------------------------------\n",
      "> Fold 1 - Loss: 0.10807187855243683 - Accuracy: 97.49644994735718%\n",
      "-----------------------------------------------\n",
      "> Fold 2 - Loss: 0.10758189111948013 - Accuracy: 97.14082479476929%\n",
      "-----------------------------------------------\n",
      "> Fold 3 - Loss: 0.0895446315407753 - Accuracy: 97.69135117530823%\n",
      "-----------------------------------------------\n",
      "Average Metrics for all folds: \n",
      "> Accuracy: 97.44287530581157 (+- 0.22792177836541833)\n",
      "> Loss: 0.10173280040423076\n",
      "-----------------------------------------------\n",
      "************************************************\n",
      "For subject 16 without noise\n",
      "The loss is 0.08285621553659439 and accuracy is 97.75094985961914%\n",
      "For subject 16 and noise 5 dB\n",
      "The loss is 0.1429429054260254 and accuracy is 96.75663113594055%\n",
      "For subject 16 and noise 10 dB\n",
      "The loss is 0.14094404876232147 and accuracy is 96.80397510528564%\n",
      "For subject 16 and noise 15 dB\n",
      "The loss is 0.15044760704040527 and accuracy is 96.75663113594055%\n",
      "************************************************\n",
      "####################################################################################\n",
      "Loading subject 17\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (17408, 128, 102, 1)\n",
      "The input label shape is (17408,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Adding noise to RAW input test data\n",
      "Size of the input test data is (4352, 128, 102, 1)\n",
      "---------------------------------------------------\n",
      "Training for fold 1 -------\n",
      "Epoch 1/150\n",
      "323/323 [==============================] - 69s 151ms/step - loss: 4.1790 - accuracy: 0.0295\n",
      "Epoch 2/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.2605 - accuracy: 0.0973\n",
      "Epoch 3/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 2.3893 - accuracy: 0.2930\n",
      "Epoch 4/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 1.6751 - accuracy: 0.5015\n",
      "Epoch 5/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.2483 - accuracy: 0.6383\n",
      "Epoch 6/150\n",
      "323/323 [==============================] - 50s 154ms/step - loss: 0.9846 - accuracy: 0.7212\n",
      "Epoch 7/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.7822 - accuracy: 0.7788\n",
      "Epoch 8/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.6588 - accuracy: 0.8133\n",
      "Epoch 9/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.5760 - accuracy: 0.8382\n",
      "Epoch 10/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4813 - accuracy: 0.8614\n",
      "Epoch 11/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4124 - accuracy: 0.8806\n",
      "Epoch 12/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.3709 - accuracy: 0.8902\n",
      "Epoch 13/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3457 - accuracy: 0.8985\n",
      "Epoch 14/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.2984 - accuracy: 0.9105\n",
      "Epoch 15/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2781 - accuracy: 0.9180\n",
      "Epoch 16/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.2708 - accuracy: 0.9185\n",
      "Epoch 17/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2501 - accuracy: 0.9264\n",
      "Epoch 18/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2265 - accuracy: 0.9324\n",
      "Epoch 19/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2210 - accuracy: 0.9337\n",
      "Epoch 20/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2140 - accuracy: 0.9359\n",
      "Epoch 21/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1896 - accuracy: 0.9416\n",
      "Epoch 22/150\n",
      "323/323 [==============================] - 50s 153ms/step - loss: 0.1837 - accuracy: 0.9438\n",
      "Epoch 23/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1916 - accuracy: 0.9404\n",
      "Epoch 24/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1413 - accuracy: 0.9587\n",
      "Epoch 25/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1739 - accuracy: 0.9492\n",
      "Epoch 26/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1580 - accuracy: 0.9521\n",
      "Epoch 27/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1391 - accuracy: 0.9568\n",
      "Epoch 28/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1450 - accuracy: 0.9542\n",
      "Epoch 29/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1473 - accuracy: 0.9539\n",
      "Epoch 30/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1373 - accuracy: 0.9579\n",
      "Epoch 31/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1232 - accuracy: 0.9616\n",
      "Epoch 32/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1115 - accuracy: 0.9673\n",
      "Epoch 33/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1243 - accuracy: 0.9602\n",
      "Epoch 34/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1262 - accuracy: 0.9603\n",
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1040 - accuracy: 0.9683\n",
      "Epoch 36/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0786 - accuracy: 0.9751\n",
      "Epoch 37/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0865 - accuracy: 0.9735\n",
      "Epoch 38/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1034 - accuracy: 0.9678\n",
      "Epoch 39/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0631 - accuracy: 0.9817\n",
      "Epoch 40/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0875 - accuracy: 0.9717\n",
      "Epoch 41/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0827 - accuracy: 0.9749\n",
      "Epoch 42/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0485 - accuracy: 0.9847\n",
      "Epoch 43/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0631 - accuracy: 0.9818\n",
      "Epoch 44/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0709 - accuracy: 0.9779\n",
      "Epoch 45/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0727 - accuracy: 0.9760\n",
      "Epoch 46/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0734 - accuracy: 0.9787\n",
      "Epoch 47/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0669 - accuracy: 0.9797\n",
      "Epoch 48/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0541 - accuracy: 0.9845\n",
      "Epoch 49/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0297 - accuracy: 0.9919\n",
      "Epoch 50/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0676 - accuracy: 0.9782\n",
      "Epoch 51/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0645 - accuracy: 0.9810\n",
      "Epoch 52/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0614 - accuracy: 0.9822\n",
      "Epoch 53/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0428 - accuracy: 0.9873\n",
      "Epoch 54/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0315 - accuracy: 0.9910\n",
      "Epoch 55/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0714 - accuracy: 0.9785\n",
      "Epoch 56/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0391 - accuracy: 0.9888\n",
      "Epoch 57/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0465 - accuracy: 0.9872\n",
      "Epoch 58/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0401 - accuracy: 0.9891\n",
      "Epoch 59/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0726 - accuracy: 0.9766\n",
      "Score for fold  1: loss of 0.14280134439468384; accuracy of 95.8642065525055%\n",
      "---------------------------------------------------\n",
      "Training for fold 2 -------\n",
      "Epoch 1/150\n",
      "323/323 [==============================] - 68s 151ms/step - loss: 4.8657 - accuracy: 0.0301\n",
      "Epoch 2/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.6201 - accuracy: 0.0277\n",
      "Epoch 3/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5897 - accuracy: 0.0286\n",
      "Epoch 4/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5737 - accuracy: 0.0302\n",
      "Epoch 5/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5027 - accuracy: 0.0410\n",
      "Epoch 6/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 2.9662 - accuracy: 0.1272\n",
      "Epoch 7/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.4319 - accuracy: 0.2769\n",
      "Epoch 8/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 2.0045 - accuracy: 0.4008\n",
      "Epoch 9/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.6259 - accuracy: 0.5108\n",
      "Epoch 10/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.3522 - accuracy: 0.5989\n",
      "Epoch 11/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.1506 - accuracy: 0.6529\n",
      "Epoch 12/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 1.0126 - accuracy: 0.6905\n",
      "Epoch 13/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.8729 - accuracy: 0.7280\n",
      "Epoch 14/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.7508 - accuracy: 0.7730\n",
      "Epoch 15/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.6345 - accuracy: 0.8045\n",
      "Epoch 16/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.5333 - accuracy: 0.8407\n",
      "Epoch 17/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4504 - accuracy: 0.8617\n",
      "Epoch 18/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.3961 - accuracy: 0.8800\n",
      "Epoch 19/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3456 - accuracy: 0.8943\n",
      "Epoch 20/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.3185 - accuracy: 0.9011\n",
      "Epoch 21/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2863 - accuracy: 0.9089\n",
      "Epoch 22/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2667 - accuracy: 0.9193\n",
      "Epoch 23/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2422 - accuracy: 0.9252\n",
      "Epoch 24/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2138 - accuracy: 0.9355\n",
      "Epoch 25/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1975 - accuracy: 0.9376\n",
      "Epoch 26/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1783 - accuracy: 0.9454\n",
      "Epoch 27/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1663 - accuracy: 0.9502\n",
      "Epoch 28/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1592 - accuracy: 0.9509\n",
      "Epoch 29/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1433 - accuracy: 0.9551\n",
      "Epoch 30/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1330 - accuracy: 0.9623\n",
      "Epoch 31/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.1234 - accuracy: 0.9623\n",
      "Epoch 32/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1129 - accuracy: 0.9660\n",
      "Epoch 33/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0996 - accuracy: 0.9684\n",
      "Epoch 34/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1008 - accuracy: 0.9699\n",
      "Epoch 35/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1037 - accuracy: 0.9703\n",
      "Epoch 36/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0779 - accuracy: 0.9778\n",
      "Epoch 37/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0721 - accuracy: 0.9783\n",
      "Epoch 38/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0784 - accuracy: 0.9753\n",
      "Epoch 39/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0777 - accuracy: 0.9777\n",
      "Epoch 40/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0741 - accuracy: 0.9776\n",
      "Epoch 41/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0609 - accuracy: 0.9819\n",
      "Epoch 42/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0583 - accuracy: 0.9838\n",
      "Epoch 43/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0662 - accuracy: 0.9810\n",
      "Epoch 44/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0664 - accuracy: 0.9800\n",
      "Epoch 45/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0552 - accuracy: 0.9854\n",
      "Epoch 46/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0652 - accuracy: 0.9821\n",
      "Epoch 47/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0413 - accuracy: 0.9911\n",
      "Epoch 48/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0461 - accuracy: 0.9872\n",
      "Epoch 49/150\n",
      "323/323 [==============================] - 50s 153ms/step - loss: 0.0728 - accuracy: 0.9805\n",
      "Epoch 50/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0515 - accuracy: 0.9859\n",
      "Epoch 51/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0412 - accuracy: 0.9887\n",
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0430 - accuracy: 0.9882\n",
      "Epoch 53/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0514 - accuracy: 0.9867\n",
      "Epoch 54/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0323 - accuracy: 0.9918\n",
      "Epoch 55/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0673 - accuracy: 0.9804\n",
      "Epoch 56/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0397 - accuracy: 0.9894\n",
      "Epoch 57/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0414 - accuracy: 0.9896\n",
      "Epoch 58/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0447 - accuracy: 0.9882\n",
      "Epoch 59/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0380 - accuracy: 0.9909\n",
      "Epoch 60/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0375 - accuracy: 0.9909\n",
      "Epoch 61/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0479 - accuracy: 0.9858\n",
      "Epoch 62/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0424 - accuracy: 0.9891\n",
      "Epoch 63/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0400 - accuracy: 0.9895\n",
      "Epoch 64/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0441 - accuracy: 0.9890\n",
      "Score for fold  2: loss of 0.07630344480276108; accuracy of 97.98380136489868%\n",
      "---------------------------------------------------\n",
      "Training for fold 3 -------\n",
      "Epoch 1/150\n",
      "323/323 [==============================] - 69s 151ms/step - loss: 4.4744 - accuracy: 0.0300\n",
      "Epoch 2/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.6103 - accuracy: 0.0257\n",
      "Epoch 3/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 3.5847 - accuracy: 0.0265\n",
      "Epoch 4/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5643 - accuracy: 0.0307\n",
      "Epoch 5/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5640 - accuracy: 0.0290\n",
      "Epoch 6/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 3.5600 - accuracy: 0.0274\n",
      "Epoch 7/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5587 - accuracy: 0.0277\n",
      "Epoch 8/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5515 - accuracy: 0.0305\n",
      "Epoch 9/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.5455 - accuracy: 0.0304\n",
      "Epoch 10/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.8918 - accuracy: 0.1523\n",
      "Epoch 11/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.2476 - accuracy: 0.3241\n",
      "Epoch 12/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.7114 - accuracy: 0.4913\n",
      "Epoch 13/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.3046 - accuracy: 0.6164\n",
      "Epoch 14/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.0447 - accuracy: 0.6936\n",
      "Epoch 15/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.8278 - accuracy: 0.7565\n",
      "Epoch 16/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.6896 - accuracy: 0.7948\n",
      "Epoch 17/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.5621 - accuracy: 0.8378\n",
      "Epoch 18/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4914 - accuracy: 0.8581\n",
      "Epoch 19/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4027 - accuracy: 0.8871\n",
      "Epoch 20/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3591 - accuracy: 0.9013\n",
      "Epoch 21/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3266 - accuracy: 0.9075\n",
      "Epoch 22/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2727 - accuracy: 0.9262\n",
      "Epoch 23/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2536 - accuracy: 0.9301\n",
      "Epoch 24/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2314 - accuracy: 0.9353\n",
      "Epoch 25/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2055 - accuracy: 0.9454\n",
      "Epoch 26/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1828 - accuracy: 0.9513\n",
      "Epoch 27/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1621 - accuracy: 0.9580\n",
      "Epoch 28/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1521 - accuracy: 0.9598\n",
      "Epoch 29/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1449 - accuracy: 0.9626\n",
      "Epoch 30/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1384 - accuracy: 0.9642\n",
      "Epoch 31/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.1283 - accuracy: 0.9677\n",
      "Epoch 32/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1161 - accuracy: 0.9703\n",
      "Epoch 33/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1021 - accuracy: 0.9754\n",
      "Epoch 34/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0949 - accuracy: 0.9786\n",
      "Epoch 35/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0975 - accuracy: 0.9773\n",
      "Epoch 36/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0955 - accuracy: 0.9785\n",
      "Epoch 37/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1007 - accuracy: 0.9782\n",
      "Epoch 38/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.1002 - accuracy: 0.9757\n",
      "Epoch 39/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0918 - accuracy: 0.9793\n",
      "Epoch 40/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0869 - accuracy: 0.9810\n",
      "Epoch 41/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0785 - accuracy: 0.9842\n",
      "Epoch 42/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0794 - accuracy: 0.9830\n",
      "Epoch 43/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0906 - accuracy: 0.9792\n",
      "Epoch 44/150\n",
      "323/323 [==============================] - 50s 153ms/step - loss: 0.0843 - accuracy: 0.9829\n",
      "Epoch 45/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0854 - accuracy: 0.9813\n",
      "Epoch 46/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0744 - accuracy: 0.9864\n",
      "Epoch 47/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0800 - accuracy: 0.9847\n",
      "Epoch 48/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0798 - accuracy: 0.9836\n",
      "Epoch 49/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0783 - accuracy: 0.9850\n",
      "Epoch 50/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0838 - accuracy: 0.9829\n",
      "Epoch 51/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0831 - accuracy: 0.9846\n",
      "Epoch 52/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0800 - accuracy: 0.9864\n",
      "Epoch 53/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0934 - accuracy: 0.9794\n",
      "Epoch 54/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0832 - accuracy: 0.9858\n",
      "Epoch 55/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0860 - accuracy: 0.9847\n",
      "Epoch 56/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0845 - accuracy: 0.9840\n",
      "Score for fold  3: loss of 0.09316158294677734; accuracy of 97.60427474975586%\n",
      "Average Score per fold \n",
      "-----------------------------------------------\n",
      "> Fold 1 - Loss: 0.14280134439468384 - Accuracy: 95.8642065525055%\n",
      "-----------------------------------------------\n",
      "> Fold 2 - Loss: 0.07630344480276108 - Accuracy: 97.98380136489868%\n",
      "-----------------------------------------------\n",
      "> Fold 3 - Loss: 0.09316158294677734 - Accuracy: 97.60427474975586%\n",
      "-----------------------------------------------\n",
      "Average Metrics for all folds: \n",
      "> Accuracy: 97.15076088905334 (+- 0.9228313889214373)\n",
      "> Loss: 0.10408879071474075\n",
      "-----------------------------------------------\n",
      "************************************************\n",
      "For subject 17 without noise\n",
      "The loss is 0.06875711679458618 and accuracy is 98.16176295280457%\n",
      "For subject 17 and noise 5 dB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 0.2094370573759079 and accuracy is 96.78308963775635%\n",
      "For subject 17 and noise 10 dB\n",
      "The loss is 0.23455193638801575 and accuracy is 96.80606722831726%\n",
      "For subject 17 and noise 15 dB\n",
      "The loss is 0.2258310317993164 and accuracy is 96.46139740943909%\n",
      "************************************************\n",
      "####################################################################################\n",
      "Loading subject 18\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (17279, 128, 102, 1)\n",
      "The input label shape is (17279,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Adding noise to RAW input test data\n",
      "Size of the input test data is (4320, 128, 102, 1)\n",
      "---------------------------------------------------\n",
      "Training for fold 1 -------\n",
      "Epoch 1/150\n",
      "320/320 [==============================] - 68s 151ms/step - loss: 4.2375 - accuracy: 0.0280\n",
      "Epoch 2/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.6081 - accuracy: 0.0485\n",
      "Epoch 3/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 2.9568 - accuracy: 0.1541\n",
      "Epoch 4/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 2.2224 - accuracy: 0.3413\n",
      "Epoch 5/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 1.6701 - accuracy: 0.5071\n",
      "Epoch 6/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 1.2681 - accuracy: 0.6314\n",
      "Epoch 7/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 1.0448 - accuracy: 0.6960\n",
      "Epoch 8/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.8740 - accuracy: 0.7463\n",
      "Epoch 9/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.7523 - accuracy: 0.7785\n",
      "Epoch 10/150\n",
      "320/320 [==============================] - 48s 151ms/step - loss: 0.6504 - accuracy: 0.8067\n",
      "Epoch 11/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.5776 - accuracy: 0.8258\n",
      "Epoch 12/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.5119 - accuracy: 0.8482\n",
      "Epoch 13/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.4799 - accuracy: 0.8521\n",
      "Epoch 14/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.4082 - accuracy: 0.8759\n",
      "Epoch 15/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.3891 - accuracy: 0.8841\n",
      "Epoch 16/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.3508 - accuracy: 0.8920\n",
      "Epoch 17/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.3221 - accuracy: 0.9011\n",
      "Epoch 18/150\n",
      "320/320 [==============================] - 48s 152ms/step - loss: 0.3163 - accuracy: 0.9001\n",
      "Epoch 19/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.2698 - accuracy: 0.9186\n",
      "Epoch 20/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2716 - accuracy: 0.9175\n",
      "Epoch 21/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.2686 - accuracy: 0.9136\n",
      "Epoch 22/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2436 - accuracy: 0.9245\n",
      "Epoch 23/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.2323 - accuracy: 0.9253\n",
      "Epoch 24/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2198 - accuracy: 0.9321\n",
      "Epoch 25/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.2122 - accuracy: 0.9308\n",
      "Epoch 26/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1964 - accuracy: 0.9355\n",
      "Epoch 27/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1986 - accuracy: 0.9396\n",
      "Epoch 28/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1916 - accuracy: 0.9407\n",
      "Epoch 29/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1964 - accuracy: 0.9404\n",
      "Epoch 30/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1802 - accuracy: 0.9432\n",
      "Epoch 31/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1672 - accuracy: 0.9502\n",
      "Epoch 32/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1410 - accuracy: 0.9565\n",
      "Epoch 33/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1738 - accuracy: 0.9441\n",
      "Epoch 34/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1693 - accuracy: 0.9476\n",
      "Epoch 35/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1270 - accuracy: 0.9594\n",
      "Epoch 36/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1171 - accuracy: 0.9628\n",
      "Epoch 37/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1088 - accuracy: 0.9654\n",
      "Epoch 38/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1216 - accuracy: 0.9627\n",
      "Epoch 39/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1122 - accuracy: 0.9636\n",
      "Epoch 40/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1203 - accuracy: 0.9629\n",
      "Epoch 41/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0962 - accuracy: 0.9692\n",
      "Epoch 42/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0968 - accuracy: 0.9705\n",
      "Epoch 43/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0734 - accuracy: 0.9768\n",
      "Epoch 44/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0980 - accuracy: 0.9695\n",
      "Epoch 45/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0902 - accuracy: 0.9723\n",
      "Epoch 46/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0834 - accuracy: 0.9748\n",
      "Epoch 47/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0707 - accuracy: 0.9795\n",
      "Epoch 48/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0889 - accuracy: 0.9737\n",
      "Epoch 49/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0794 - accuracy: 0.9758\n",
      "Epoch 50/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0796 - accuracy: 0.9755\n",
      "Epoch 51/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0920 - accuracy: 0.9718\n",
      "Epoch 52/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0672 - accuracy: 0.9796\n",
      "Epoch 53/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0496 - accuracy: 0.9865\n",
      "Epoch 54/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0762 - accuracy: 0.9763\n",
      "Epoch 55/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0757 - accuracy: 0.9788\n",
      "Epoch 56/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0555 - accuracy: 0.9845\n",
      "Epoch 57/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0606 - accuracy: 0.9832\n",
      "Epoch 58/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0717 - accuracy: 0.9783\n",
      "Epoch 59/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0562 - accuracy: 0.9829\n",
      "Epoch 60/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0575 - accuracy: 0.9823\n",
      "Epoch 61/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0699 - accuracy: 0.9797\n",
      "Epoch 62/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0633 - accuracy: 0.9818\n",
      "Epoch 63/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0380 - accuracy: 0.9893\n",
      "Epoch 64/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0597 - accuracy: 0.9823\n",
      "Epoch 65/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0683 - accuracy: 0.9802\n",
      "Epoch 66/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0496 - accuracy: 0.9865\n",
      "Epoch 67/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0420 - accuracy: 0.9874\n",
      "Epoch 68/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0419 - accuracy: 0.9871\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 48s 151ms/step - loss: 0.0513 - accuracy: 0.9855\n",
      "Epoch 70/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0672 - accuracy: 0.9796\n",
      "Epoch 71/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0627 - accuracy: 0.9821\n",
      "Epoch 72/150\n",
      "320/320 [==============================] - 48s 151ms/step - loss: 0.0495 - accuracy: 0.9858\n",
      "Epoch 73/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0454 - accuracy: 0.9865\n",
      "Score for fold  1: loss of 0.10949742048978806; accuracy of 97.04861044883728%\n",
      "---------------------------------------------------\n",
      "Training for fold 2 -------\n",
      "Epoch 1/150\n",
      "320/320 [==============================] - 69s 151ms/step - loss: 4.8059 - accuracy: 0.0307\n",
      "Epoch 2/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.6410 - accuracy: 0.0294\n",
      "Epoch 3/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.6065 - accuracy: 0.0316\n",
      "Epoch 4/150\n",
      "320/320 [==============================] - 48s 151ms/step - loss: 3.5788 - accuracy: 0.0288\n",
      "Epoch 5/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.5689 - accuracy: 0.0284\n",
      "Epoch 6/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.5621 - accuracy: 0.0294\n",
      "Epoch 7/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.5566 - accuracy: 0.0286\n",
      "Epoch 8/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 3.5500 - accuracy: 0.0293\n",
      "Epoch 9/150\n",
      "320/320 [==============================] - 48s 151ms/step - loss: 3.5489 - accuracy: 0.0303\n",
      "Epoch 10/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 3.4031 - accuracy: 0.0580\n",
      "Epoch 11/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 2.7902 - accuracy: 0.1853\n",
      "Epoch 12/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 2.2028 - accuracy: 0.3346\n",
      "Epoch 13/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 1.8278 - accuracy: 0.4461\n",
      "Epoch 14/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 1.5063 - accuracy: 0.5348\n",
      "Epoch 15/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 1.2412 - accuracy: 0.6097\n",
      "Epoch 16/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 1.0345 - accuracy: 0.6757\n",
      "Epoch 17/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.8681 - accuracy: 0.7287\n",
      "Epoch 18/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.7525 - accuracy: 0.7570\n",
      "Epoch 19/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.6412 - accuracy: 0.8016\n",
      "Epoch 20/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.5606 - accuracy: 0.8267\n",
      "Epoch 21/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.4815 - accuracy: 0.8496\n",
      "Epoch 22/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.4325 - accuracy: 0.8657\n",
      "Epoch 23/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.3877 - accuracy: 0.8801\n",
      "Epoch 24/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.3407 - accuracy: 0.8990\n",
      "Epoch 25/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.3193 - accuracy: 0.9016\n",
      "Epoch 26/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.2936 - accuracy: 0.9115\n",
      "Epoch 27/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2650 - accuracy: 0.9192\n",
      "Epoch 28/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2313 - accuracy: 0.9325\n",
      "Epoch 29/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2228 - accuracy: 0.9347\n",
      "Epoch 30/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.2041 - accuracy: 0.9427\n",
      "Epoch 31/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1938 - accuracy: 0.9451\n",
      "Epoch 32/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1807 - accuracy: 0.9480\n",
      "Epoch 33/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1773 - accuracy: 0.9523\n",
      "Epoch 34/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1526 - accuracy: 0.9580\n",
      "Epoch 35/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1654 - accuracy: 0.9525\n",
      "Epoch 36/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1421 - accuracy: 0.9605\n",
      "Epoch 37/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1357 - accuracy: 0.9619\n",
      "Epoch 38/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1266 - accuracy: 0.9651\n",
      "Epoch 39/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1159 - accuracy: 0.9695\n",
      "Epoch 40/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.1276 - accuracy: 0.9644\n",
      "Epoch 41/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.1101 - accuracy: 0.9706\n",
      "Epoch 42/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0935 - accuracy: 0.9740\n",
      "Epoch 43/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0995 - accuracy: 0.9719\n",
      "Epoch 44/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0805 - accuracy: 0.9797\n",
      "Epoch 45/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0803 - accuracy: 0.9786\n",
      "Epoch 46/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0826 - accuracy: 0.9779\n",
      "Epoch 47/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0833 - accuracy: 0.9780\n",
      "Epoch 48/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0916 - accuracy: 0.9767\n",
      "Epoch 49/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0681 - accuracy: 0.9820\n",
      "Epoch 50/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0609 - accuracy: 0.9849\n",
      "Epoch 51/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0577 - accuracy: 0.9862\n",
      "Epoch 52/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0632 - accuracy: 0.9838\n",
      "Epoch 53/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0765 - accuracy: 0.9790\n",
      "Epoch 54/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0536 - accuracy: 0.9869\n",
      "Epoch 55/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0586 - accuracy: 0.9844\n",
      "Epoch 56/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0437 - accuracy: 0.9907\n",
      "Epoch 57/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0639 - accuracy: 0.9832\n",
      "Epoch 58/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0552 - accuracy: 0.9873\n",
      "Epoch 59/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0513 - accuracy: 0.9885\n",
      "Epoch 60/150\n",
      "320/320 [==============================] - 49s 153ms/step - loss: 0.0513 - accuracy: 0.9875\n",
      "Epoch 61/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0492 - accuracy: 0.9898\n",
      "Epoch 62/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0545 - accuracy: 0.9867\n",
      "Epoch 63/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0469 - accuracy: 0.9885\n",
      "Epoch 64/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0568 - accuracy: 0.9857\n",
      "Epoch 65/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0430 - accuracy: 0.9906\n",
      "Epoch 66/150\n",
      "320/320 [==============================] - 49s 152ms/step - loss: 0.0552 - accuracy: 0.9874\n",
      "Score for fold  2: loss of 0.0878174751996994; accuracy of 97.51735925674438%\n",
      "---------------------------------------------------\n",
      "Training for fold 3 -------\n",
      "Epoch 1/150\n",
      "320/320 [==============================] - 56s 126ms/step - loss: 4.0926 - accuracy: 0.0275\n",
      "Epoch 2/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 3.5618 - accuracy: 0.0404\n",
      "Epoch 3/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 3.4124 - accuracy: 0.0595\n",
      "Epoch 4/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 3.2034 - accuracy: 0.0919\n",
      "Epoch 5/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 40s 126ms/step - loss: 2.9078 - accuracy: 0.1444\n",
      "Epoch 6/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 2.5825 - accuracy: 0.2199\n",
      "Epoch 7/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 2.1252 - accuracy: 0.3595\n",
      "Epoch 8/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 1.6658 - accuracy: 0.5076\n",
      "Epoch 9/150\n",
      "320/320 [==============================] - 41s 127ms/step - loss: 1.2887 - accuracy: 0.6272\n",
      "Epoch 10/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 1.0463 - accuracy: 0.7026\n",
      "Epoch 11/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.8640 - accuracy: 0.7519\n",
      "Epoch 12/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.7376 - accuracy: 0.7901\n",
      "Epoch 13/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.6406 - accuracy: 0.8174\n",
      "Epoch 14/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.5497 - accuracy: 0.8425\n",
      "Epoch 15/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.4790 - accuracy: 0.8626\n",
      "Epoch 16/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.4228 - accuracy: 0.8800\n",
      "Epoch 17/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.3808 - accuracy: 0.8918\n",
      "Epoch 18/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.3332 - accuracy: 0.9082\n",
      "Epoch 19/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.3015 - accuracy: 0.9166\n",
      "Epoch 20/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.2555 - accuracy: 0.9312\n",
      "Epoch 21/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.2421 - accuracy: 0.9377\n",
      "Epoch 22/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.2219 - accuracy: 0.9448\n",
      "Epoch 23/150\n",
      "320/320 [==============================] - 41s 127ms/step - loss: 0.2054 - accuracy: 0.9485\n",
      "Epoch 24/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1930 - accuracy: 0.9523\n",
      "Epoch 25/150\n",
      "320/320 [==============================] - 41s 127ms/step - loss: 0.1739 - accuracy: 0.9542\n",
      "Epoch 26/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1749 - accuracy: 0.9565\n",
      "Epoch 27/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1618 - accuracy: 0.9625\n",
      "Epoch 28/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1555 - accuracy: 0.9614\n",
      "Epoch 29/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1426 - accuracy: 0.9681\n",
      "Epoch 30/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1411 - accuracy: 0.9672\n",
      "Epoch 31/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1302 - accuracy: 0.9733\n",
      "Epoch 32/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1353 - accuracy: 0.9707\n",
      "Epoch 33/150\n",
      "320/320 [==============================] - 40s 125ms/step - loss: 0.1444 - accuracy: 0.9648\n",
      "Epoch 34/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1320 - accuracy: 0.9706\n",
      "Epoch 35/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1261 - accuracy: 0.9738\n",
      "Epoch 36/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1327 - accuracy: 0.9739\n",
      "Epoch 37/150\n",
      "320/320 [==============================] - 40s 127ms/step - loss: 0.1294 - accuracy: 0.9715\n",
      "Epoch 38/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1240 - accuracy: 0.9724\n",
      "Epoch 39/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1252 - accuracy: 0.9746\n",
      "Epoch 40/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1292 - accuracy: 0.9755\n",
      "Epoch 41/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1185 - accuracy: 0.9781\n",
      "Epoch 42/150\n",
      "320/320 [==============================] - 40s 125ms/step - loss: 0.1297 - accuracy: 0.9730\n",
      "Epoch 43/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1341 - accuracy: 0.9738\n",
      "Epoch 44/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1330 - accuracy: 0.9751\n",
      "Epoch 45/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1403 - accuracy: 0.9724\n",
      "Epoch 46/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1391 - accuracy: 0.9715\n",
      "Epoch 47/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1407 - accuracy: 0.9746\n",
      "Epoch 48/150\n",
      "320/320 [==============================] - 40s 125ms/step - loss: 0.1426 - accuracy: 0.9737\n",
      "Epoch 49/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1498 - accuracy: 0.9729\n",
      "Epoch 50/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1528 - accuracy: 0.9707\n",
      "Epoch 51/150\n",
      "320/320 [==============================] - 40s 126ms/step - loss: 0.1562 - accuracy: 0.9731\n",
      "Score for fold  3: loss of 0.14474184811115265; accuracy of 95.95415592193604%\n",
      "Average Score per fold \n",
      "-----------------------------------------------\n",
      "> Fold 1 - Loss: 0.10949742048978806 - Accuracy: 97.04861044883728%\n",
      "-----------------------------------------------\n",
      "> Fold 2 - Loss: 0.0878174751996994 - Accuracy: 97.51735925674438%\n",
      "-----------------------------------------------\n",
      "> Fold 3 - Loss: 0.14474184811115265 - Accuracy: 95.95415592193604%\n",
      "-----------------------------------------------\n",
      "Average Metrics for all folds: \n",
      "> Accuracy: 96.84004187583923 (+- 0.654994556578306)\n",
      "> Loss: 0.11401891460021336\n",
      "-----------------------------------------------\n",
      "************************************************\n",
      "For subject 18 without noise\n",
      "The loss is 0.14292044937610626 and accuracy is 95.92592716217041%\n",
      "For subject 18 and noise 5 dB\n",
      "The loss is 0.22476570308208466 and accuracy is 94.81481313705444%\n",
      "For subject 18 and noise 10 dB\n",
      "The loss is 0.23319172859191895 and accuracy is 94.74536776542664%\n",
      "For subject 18 and noise 15 dB\n",
      "The loss is 0.23455731570720673 and accuracy is 94.6759283542633%\n",
      "************************************************\n",
      "####################################################################################\n",
      "Loading subject 19\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (17023, 128, 102, 1)\n",
      "The input label shape is (17023,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Adding noise to RAW input test data\n",
      "Size of the input test data is (4256, 128, 102, 1)\n",
      "---------------------------------------------------\n",
      "Training for fold 1 -------\n",
      "Epoch 1/150\n",
      "316/316 [==============================] - 67s 151ms/step - loss: 4.2369 - accuracy: 0.0310\n",
      "Epoch 2/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 3.5920 - accuracy: 0.0589\n",
      "Epoch 3/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 2.6302 - accuracy: 0.2467\n",
      "Epoch 4/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 1.8277 - accuracy: 0.4529\n",
      "Epoch 5/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 1.4226 - accuracy: 0.5827\n",
      "Epoch 6/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 1.1450 - accuracy: 0.6658\n",
      "Epoch 7/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.9832 - accuracy: 0.7113\n",
      "Epoch 8/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.8560 - accuracy: 0.7486\n",
      "Epoch 9/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.7553 - accuracy: 0.7741\n",
      "Epoch 10/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.6734 - accuracy: 0.8001\n",
      "Epoch 11/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.6139 - accuracy: 0.8158\n",
      "Epoch 12/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.5481 - accuracy: 0.8324\n",
      "Epoch 13/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.5366 - accuracy: 0.8376\n",
      "Epoch 14/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 48s 152ms/step - loss: 0.4881 - accuracy: 0.8475\n",
      "Epoch 15/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.4244 - accuracy: 0.8705\n",
      "Epoch 16/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.4136 - accuracy: 0.8705\n",
      "Epoch 17/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.3855 - accuracy: 0.8818\n",
      "Epoch 18/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.3806 - accuracy: 0.8796\n",
      "Epoch 19/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.3436 - accuracy: 0.8918\n",
      "Epoch 20/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.3298 - accuracy: 0.9010\n",
      "Epoch 21/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.3106 - accuracy: 0.8988\n",
      "Epoch 22/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.3218 - accuracy: 0.8964\n",
      "Epoch 23/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.2751 - accuracy: 0.9159\n",
      "Epoch 24/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.2420 - accuracy: 0.9256\n",
      "Epoch 25/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.2721 - accuracy: 0.9106\n",
      "Epoch 26/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.2502 - accuracy: 0.9171\n",
      "Epoch 27/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.2447 - accuracy: 0.9216\n",
      "Epoch 28/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.2172 - accuracy: 0.9292\n",
      "Epoch 29/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.2348 - accuracy: 0.9272\n",
      "Epoch 30/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1918 - accuracy: 0.9410\n",
      "Epoch 31/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.2263 - accuracy: 0.9282\n",
      "Epoch 32/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1691 - accuracy: 0.9455\n",
      "Epoch 33/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1724 - accuracy: 0.9447\n",
      "Epoch 34/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1859 - accuracy: 0.9395\n",
      "Epoch 35/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1359 - accuracy: 0.9565\n",
      "Epoch 36/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1492 - accuracy: 0.9515\n",
      "Epoch 37/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.1447 - accuracy: 0.9532\n",
      "Epoch 38/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.1200 - accuracy: 0.9627\n",
      "Epoch 39/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.1240 - accuracy: 0.9625\n",
      "Epoch 40/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.1361 - accuracy: 0.9555\n",
      "Epoch 41/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.1144 - accuracy: 0.9658\n",
      "Epoch 42/150\n",
      "316/316 [==============================] - 52s 164ms/step - loss: 0.1227 - accuracy: 0.9614\n",
      "Epoch 43/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0927 - accuracy: 0.9723\n",
      "Epoch 44/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.1090 - accuracy: 0.9658\n",
      "Epoch 45/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0952 - accuracy: 0.9712\n",
      "Epoch 46/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0843 - accuracy: 0.9749\n",
      "Epoch 47/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0694 - accuracy: 0.9800\n",
      "Epoch 48/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.1163 - accuracy: 0.9630\n",
      "Epoch 49/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0833 - accuracy: 0.9758\n",
      "Epoch 50/150\n",
      "316/316 [==============================] - 52s 163ms/step - loss: 0.0843 - accuracy: 0.9743\n",
      "Epoch 51/150\n",
      "316/316 [==============================] - 54s 170ms/step - loss: 0.0631 - accuracy: 0.9815\n",
      "Epoch 52/150\n",
      "316/316 [==============================] - 54s 171ms/step - loss: 0.1142 - accuracy: 0.9626\n",
      "Epoch 53/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0783 - accuracy: 0.9750\n",
      "Epoch 54/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0697 - accuracy: 0.9789\n",
      "Epoch 55/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0687 - accuracy: 0.9803\n",
      "Epoch 56/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0674 - accuracy: 0.9792\n",
      "Epoch 57/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0630 - accuracy: 0.9827\n",
      "Epoch 58/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0740 - accuracy: 0.9754\n",
      "Epoch 59/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0802 - accuracy: 0.9751\n",
      "Epoch 60/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0693 - accuracy: 0.9795\n",
      "Epoch 61/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0564 - accuracy: 0.9818\n",
      "Epoch 62/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0559 - accuracy: 0.9840\n",
      "Epoch 63/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 0.0646 - accuracy: 0.9818\n",
      "Epoch 64/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0720 - accuracy: 0.9776\n",
      "Epoch 65/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0671 - accuracy: 0.9804\n",
      "Epoch 66/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 0.0495 - accuracy: 0.9858\n",
      "Epoch 67/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 0.0463 - accuracy: 0.9857\n",
      "Epoch 68/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0702 - accuracy: 0.9789\n",
      "Epoch 69/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0585 - accuracy: 0.9824\n",
      "Epoch 70/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0437 - accuracy: 0.9874\n",
      "Epoch 71/150\n",
      "316/316 [==============================] - 52s 163ms/step - loss: 0.0674 - accuracy: 0.9804\n",
      "Epoch 72/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0710 - accuracy: 0.9781\n",
      "Epoch 73/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0401 - accuracy: 0.9896\n",
      "Epoch 74/150\n",
      "316/316 [==============================] - 52s 163ms/step - loss: 0.0431 - accuracy: 0.9884\n",
      "Epoch 75/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0470 - accuracy: 0.9871\n",
      "Epoch 76/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0622 - accuracy: 0.9824\n",
      "Epoch 77/150\n",
      "316/316 [==============================] - 49s 157ms/step - loss: 0.0569 - accuracy: 0.9822\n",
      "Epoch 78/150\n",
      "316/316 [==============================] - 53s 167ms/step - loss: 0.0339 - accuracy: 0.9912\n",
      "Epoch 79/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0522 - accuracy: 0.9843\n",
      "Epoch 80/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0652 - accuracy: 0.9792\n",
      "Epoch 81/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0426 - accuracy: 0.9889\n",
      "Epoch 82/150\n",
      "316/316 [==============================] - 50s 160ms/step - loss: 0.0500 - accuracy: 0.9848\n",
      "Epoch 83/150\n",
      "316/316 [==============================] - 50s 160ms/step - loss: 0.0344 - accuracy: 0.9914\n",
      "Epoch 84/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0586 - accuracy: 0.9833\n",
      "Epoch 85/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0373 - accuracy: 0.9900\n",
      "Epoch 86/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0366 - accuracy: 0.9903\n",
      "Epoch 87/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0359 - accuracy: 0.9902\n",
      "Epoch 88/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0578 - accuracy: 0.9839\n",
      "Epoch 89/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0619 - accuracy: 0.9820\n",
      "Epoch 90/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0315 - accuracy: 0.9921\n",
      "Epoch 91/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0379 - accuracy: 0.9901\n",
      "Epoch 92/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0558 - accuracy: 0.9834\n",
      "Epoch 93/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0388 - accuracy: 0.9886\n",
      "Epoch 94/150\n",
      "316/316 [==============================] - 49s 157ms/step - loss: 0.0480 - accuracy: 0.9858\n",
      "Epoch 95/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0280 - accuracy: 0.9927\n",
      "Epoch 96/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0502 - accuracy: 0.9862\n",
      "Epoch 97/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0388 - accuracy: 0.9905\n",
      "Epoch 98/150\n",
      "316/316 [==============================] - 53s 169ms/step - loss: 0.0350 - accuracy: 0.9909\n",
      "Epoch 99/150\n",
      "316/316 [==============================] - 51s 163ms/step - loss: 0.0390 - accuracy: 0.9890\n",
      "Epoch 100/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0465 - accuracy: 0.9861\n",
      "Epoch 101/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0334 - accuracy: 0.9916\n",
      "Epoch 102/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0413 - accuracy: 0.9892\n",
      "Epoch 103/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0380 - accuracy: 0.9899\n",
      "Epoch 104/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0443 - accuracy: 0.9879\n",
      "Epoch 105/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0409 - accuracy: 0.9894\n",
      "Score for fold  1: loss of 0.12242056429386139; accuracy of 96.37004137039185%\n",
      "---------------------------------------------------\n",
      "Training for fold 2 -------\n",
      "Epoch 1/150\n",
      "316/316 [==============================] - 72s 158ms/step - loss: 4.6262 - accuracy: 0.0290\n",
      "Epoch 2/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 3.6080 - accuracy: 0.0300\n",
      "Epoch 3/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 3.5707 - accuracy: 0.0309\n",
      "Epoch 4/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 3.4762 - accuracy: 0.0523\n",
      "Epoch 5/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 2.7952 - accuracy: 0.1838\n",
      "Epoch 6/150\n",
      "316/316 [==============================] - 49s 157ms/step - loss: 2.2183 - accuracy: 0.3280\n",
      "Epoch 7/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 1.6651 - accuracy: 0.4776\n",
      "Epoch 8/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 1.3057 - accuracy: 0.5765\n",
      "Epoch 9/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 1.1040 - accuracy: 0.6440\n",
      "Epoch 10/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.9357 - accuracy: 0.7056\n",
      "Epoch 11/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.8134 - accuracy: 0.7449\n",
      "Epoch 12/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.7334 - accuracy: 0.7700\n",
      "Epoch 13/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.6399 - accuracy: 0.8013\n",
      "Epoch 14/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.5700 - accuracy: 0.8234\n",
      "Epoch 15/150\n",
      "316/316 [==============================] - 52s 165ms/step - loss: 0.5286 - accuracy: 0.8351\n",
      "Epoch 16/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.4570 - accuracy: 0.8595\n",
      "Epoch 17/150\n",
      "316/316 [==============================] - 49s 157ms/step - loss: 0.4358 - accuracy: 0.8677\n",
      "Epoch 18/150\n",
      "316/316 [==============================] - 51s 163ms/step - loss: 0.4052 - accuracy: 0.8786\n",
      "Epoch 19/150\n",
      "316/316 [==============================] - 59s 186ms/step - loss: 0.3681 - accuracy: 0.8855\n",
      "Epoch 20/150\n",
      "316/316 [==============================] - 57s 181ms/step - loss: 0.3349 - accuracy: 0.8985\n",
      "Epoch 21/150\n",
      "316/316 [==============================] - 59s 187ms/step - loss: 0.3197 - accuracy: 0.9017\n",
      "Epoch 22/150\n",
      "316/316 [==============================] - 57s 180ms/step - loss: 0.2969 - accuracy: 0.9104\n",
      "Epoch 23/150\n",
      "316/316 [==============================] - 58s 183ms/step - loss: 0.2826 - accuracy: 0.9159\n",
      "Epoch 24/150\n",
      "316/316 [==============================] - 56s 177ms/step - loss: 0.2560 - accuracy: 0.9224\n",
      "Epoch 25/150\n",
      "316/316 [==============================] - 59s 186ms/step - loss: 0.2418 - accuracy: 0.9303\n",
      "Epoch 26/150\n",
      "316/316 [==============================] - 59s 187ms/step - loss: 0.2282 - accuracy: 0.9337\n",
      "Epoch 27/150\n",
      "316/316 [==============================] - 57s 179ms/step - loss: 0.1967 - accuracy: 0.9434\n",
      "Epoch 28/150\n",
      "316/316 [==============================] - 54s 171ms/step - loss: 0.1967 - accuracy: 0.9431\n",
      "Epoch 29/150\n",
      "316/316 [==============================] - 51s 162ms/step - loss: 0.1831 - accuracy: 0.9495\n",
      "Epoch 30/150\n",
      "316/316 [==============================] - 52s 164ms/step - loss: 0.1612 - accuracy: 0.9539\n",
      "Epoch 31/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.1709 - accuracy: 0.9515\n",
      "Epoch 32/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.1687 - accuracy: 0.9515\n",
      "Epoch 33/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.1407 - accuracy: 0.9607\n",
      "Epoch 34/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.1465 - accuracy: 0.9593\n",
      "Epoch 35/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.1423 - accuracy: 0.9596\n",
      "Epoch 36/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.1161 - accuracy: 0.9702\n",
      "Epoch 37/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.1311 - accuracy: 0.9629\n",
      "Epoch 38/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.1186 - accuracy: 0.9679\n",
      "Epoch 39/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.1038 - accuracy: 0.9738\n",
      "Epoch 40/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.1284 - accuracy: 0.9641\n",
      "Epoch 41/150\n",
      "316/316 [==============================] - 54s 170ms/step - loss: 0.1089 - accuracy: 0.9711\n",
      "Epoch 42/150\n",
      "316/316 [==============================] - 56s 178ms/step - loss: 0.0798 - accuracy: 0.9802\n",
      "Epoch 43/150\n",
      "316/316 [==============================] - 54s 172ms/step - loss: 0.1035 - accuracy: 0.9737\n",
      "Epoch 44/150\n",
      "316/316 [==============================] - 51s 163ms/step - loss: 0.1060 - accuracy: 0.9708\n",
      "Epoch 45/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0943 - accuracy: 0.9773\n",
      "Epoch 46/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.0938 - accuracy: 0.9758\n",
      "Epoch 47/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.0948 - accuracy: 0.9759\n",
      "Epoch 48/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0949 - accuracy: 0.9753\n",
      "Epoch 49/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0825 - accuracy: 0.9787\n",
      "Epoch 50/150\n",
      "316/316 [==============================] - 52s 163ms/step - loss: 0.0819 - accuracy: 0.9811\n",
      "Epoch 51/150\n",
      "316/316 [==============================] - 56s 178ms/step - loss: 0.0872 - accuracy: 0.9775\n",
      "Epoch 52/150\n",
      "316/316 [==============================] - 51s 161ms/step - loss: 0.0940 - accuracy: 0.9765\n",
      "Epoch 53/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.0947 - accuracy: 0.9765\n",
      "Epoch 54/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0864 - accuracy: 0.9789\n",
      "Epoch 55/150\n",
      "316/316 [==============================] - 51s 163ms/step - loss: 0.0736 - accuracy: 0.9848\n",
      "Epoch 56/150\n",
      "316/316 [==============================] - 52s 166ms/step - loss: 0.0891 - accuracy: 0.9770\n",
      "Epoch 57/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.0774 - accuracy: 0.9825\n",
      "Epoch 58/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.0786 - accuracy: 0.9798\n",
      "Epoch 59/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.0901 - accuracy: 0.9780\n",
      "Epoch 60/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0846 - accuracy: 0.9796\n",
      "Epoch 61/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0848 - accuracy: 0.9807\n",
      "Epoch 62/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.0959 - accuracy: 0.9766\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 49s 154ms/step - loss: 0.0839 - accuracy: 0.9805\n",
      "Epoch 64/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.0785 - accuracy: 0.9837\n",
      "Epoch 65/150\n",
      "316/316 [==============================] - 47s 150ms/step - loss: 0.1031 - accuracy: 0.9741\n",
      "Score for fold  2: loss of 0.1340254843235016; accuracy of 95.98166942596436%\n",
      "---------------------------------------------------\n",
      "Training for fold 3 -------\n",
      "Epoch 1/150\n",
      "316/316 [==============================] - 70s 154ms/step - loss: 3.8002 - accuracy: 0.0457\n",
      "Epoch 2/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 2.7156 - accuracy: 0.2182\n",
      "Epoch 3/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 1.6268 - accuracy: 0.5233\n",
      "Epoch 4/150\n",
      "316/316 [==============================] - 53s 169ms/step - loss: 1.1813 - accuracy: 0.6637\n",
      "Epoch 5/150\n",
      "316/316 [==============================] - 51s 162ms/step - loss: 0.9444 - accuracy: 0.7343\n",
      "Epoch 6/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.7974 - accuracy: 0.7745\n",
      "Epoch 7/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.6961 - accuracy: 0.8014\n",
      "Epoch 8/150\n",
      "316/316 [==============================] - 51s 160ms/step - loss: 0.6012 - accuracy: 0.8310\n",
      "Epoch 9/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.5437 - accuracy: 0.8451\n",
      "Epoch 10/150\n",
      "316/316 [==============================] - 48s 153ms/step - loss: 0.5002 - accuracy: 0.8557\n",
      "Epoch 11/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.4505 - accuracy: 0.8736\n",
      "Epoch 12/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.4029 - accuracy: 0.8871\n",
      "Epoch 13/150\n",
      "316/316 [==============================] - 48s 151ms/step - loss: 0.3918 - accuracy: 0.8862\n",
      "Epoch 14/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.3638 - accuracy: 0.8982\n",
      "Epoch 15/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.3312 - accuracy: 0.9109\n",
      "Epoch 16/150\n",
      "316/316 [==============================] - 50s 159ms/step - loss: 0.3255 - accuracy: 0.9085\n",
      "Epoch 17/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.3167 - accuracy: 0.9143\n",
      "Epoch 18/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.2989 - accuracy: 0.9198\n",
      "Epoch 19/150\n",
      "316/316 [==============================] - 56s 177ms/step - loss: 0.2876 - accuracy: 0.9233\n",
      "Epoch 20/150\n",
      "316/316 [==============================] - 51s 163ms/step - loss: 0.2732 - accuracy: 0.9302\n",
      "Epoch 21/150\n",
      "316/316 [==============================] - 53s 168ms/step - loss: 0.2759 - accuracy: 0.9264\n",
      "Epoch 22/150\n",
      "316/316 [==============================] - 53s 167ms/step - loss: 0.2654 - accuracy: 0.9325\n",
      "Epoch 23/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.2543 - accuracy: 0.9371\n",
      "Epoch 24/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.2679 - accuracy: 0.9329\n",
      "Epoch 25/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.2565 - accuracy: 0.9366\n",
      "Epoch 26/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.2627 - accuracy: 0.9360\n",
      "Epoch 27/150\n",
      "316/316 [==============================] - 53s 168ms/step - loss: 0.2580 - accuracy: 0.9382\n",
      "Epoch 28/150\n",
      "316/316 [==============================] - 51s 163ms/step - loss: 0.2607 - accuracy: 0.9383\n",
      "Epoch 29/150\n",
      "316/316 [==============================] - 49s 155ms/step - loss: 0.2684 - accuracy: 0.9365\n",
      "Epoch 30/150\n",
      "316/316 [==============================] - 51s 162ms/step - loss: 0.2723 - accuracy: 0.9389\n",
      "Epoch 31/150\n",
      "316/316 [==============================] - 49s 154ms/step - loss: 0.2759 - accuracy: 0.9361\n",
      "Epoch 32/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.2863 - accuracy: 0.9340\n",
      "Epoch 33/150\n",
      "316/316 [==============================] - 50s 157ms/step - loss: 0.2911 - accuracy: 0.9309\n",
      "Epoch 34/150\n",
      "316/316 [==============================] - 49s 156ms/step - loss: 0.3027 - accuracy: 0.9320\n",
      "Epoch 35/150\n",
      "316/316 [==============================] - 53s 169ms/step - loss: 0.3109 - accuracy: 0.9317\n",
      "Epoch 36/150\n",
      "316/316 [==============================] - 52s 166ms/step - loss: 0.3290 - accuracy: 0.9306\n",
      "Epoch 37/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.3424 - accuracy: 0.9258\n",
      "Epoch 38/150\n",
      "316/316 [==============================] - 50s 158ms/step - loss: 0.3630 - accuracy: 0.9218\n",
      "Epoch 39/150\n",
      "316/316 [==============================] - 53s 166ms/step - loss: 0.3861 - accuracy: 0.9182\n",
      "Epoch 40/150\n",
      "316/316 [==============================] - 48s 152ms/step - loss: 0.4045 - accuracy: 0.9196\n",
      "Score for fold  3: loss of 0.33954140543937683; accuracy of 90.58864712715149%\n",
      "Average Score per fold \n",
      "-----------------------------------------------\n",
      "> Fold 1 - Loss: 0.12242056429386139 - Accuracy: 96.37004137039185%\n",
      "-----------------------------------------------\n",
      "> Fold 2 - Loss: 0.1340254843235016 - Accuracy: 95.98166942596436%\n",
      "-----------------------------------------------\n",
      "> Fold 3 - Loss: 0.33954140543937683 - Accuracy: 90.58864712715149%\n",
      "-----------------------------------------------\n",
      "Average Metrics for all folds: \n",
      "> Accuracy: 94.31345264116923 (+- 2.638603201158689)\n",
      "> Loss: 0.19866248468557993\n",
      "-----------------------------------------------\n",
      "************************************************\n",
      "For subject 19 without noise\n",
      "The loss is 0.34402093291282654 and accuracy is 90.5075192451477%\n",
      "For subject 19 and noise 5 dB\n",
      "The loss is 0.395207941532135 and accuracy is 89.63815569877625%\n",
      "For subject 19 and noise 10 dB\n",
      "The loss is 0.3899941146373749 and accuracy is 89.63815569877625%\n",
      "For subject 19 and noise 15 dB\n",
      "The loss is 0.4170559048652649 and accuracy is 89.02725577354431%\n",
      "************************************************\n",
      "####################################################################################\n",
      "Loading subject 20\n",
      "####################################################################################\n",
      "Centred.\n",
      "Extended.\n",
      "Size of the input data is (17408, 128, 102, 1)\n",
      "The input label shape is (17408,)\n",
      "The total number of classes is 34\n",
      "************************************************************************************\n",
      "Loaded RAW input data\n",
      "************************************************************************************\n",
      "Adding noise to RAW input test data\n",
      "Size of the input test data is (4352, 128, 102, 1)\n",
      "---------------------------------------------------\n",
      "Training for fold 1 -------\n",
      "Epoch 1/150\n",
      "323/323 [==============================] - 69s 151ms/step - loss: 4.1955 - accuracy: 0.0282\n",
      "Epoch 2/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.4448 - accuracy: 0.0809\n",
      "Epoch 3/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.5042 - accuracy: 0.2558\n",
      "Epoch 4/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.0015 - accuracy: 0.4115\n",
      "Epoch 5/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.6157 - accuracy: 0.5215\n",
      "Epoch 6/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.3293 - accuracy: 0.6157\n",
      "Epoch 7/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.1036 - accuracy: 0.6868\n",
      "Epoch 8/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.9268 - accuracy: 0.7374\n",
      "Epoch 9/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.7925 - accuracy: 0.7773\n",
      "Epoch 10/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.6728 - accuracy: 0.8085\n",
      "Epoch 11/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.5852 - accuracy: 0.8358\n",
      "Epoch 12/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.5192 - accuracy: 0.8490\n",
      "Epoch 13/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4489 - accuracy: 0.8722\n",
      "Epoch 14/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4347 - accuracy: 0.8726\n",
      "Epoch 15/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3857 - accuracy: 0.8879\n",
      "Epoch 16/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3539 - accuracy: 0.8971\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3323 - accuracy: 0.8994\n",
      "Epoch 18/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3091 - accuracy: 0.9089\n",
      "Epoch 19/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.3000 - accuracy: 0.9124\n",
      "Epoch 20/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2650 - accuracy: 0.9203\n",
      "Epoch 21/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2875 - accuracy: 0.9111\n",
      "Epoch 22/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2480 - accuracy: 0.9244\n",
      "Epoch 23/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2257 - accuracy: 0.9299\n",
      "Epoch 24/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2343 - accuracy: 0.9305\n",
      "Epoch 25/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2098 - accuracy: 0.9366\n",
      "Epoch 26/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2108 - accuracy: 0.9336\n",
      "Epoch 27/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2226 - accuracy: 0.9299\n",
      "Epoch 28/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1801 - accuracy: 0.9461\n",
      "Epoch 29/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1773 - accuracy: 0.9445\n",
      "Epoch 30/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1731 - accuracy: 0.9491\n",
      "Epoch 31/150\n",
      "323/323 [==============================] - 48s 150ms/step - loss: 0.1442 - accuracy: 0.9562\n",
      "Epoch 32/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1711 - accuracy: 0.9470\n",
      "Epoch 33/150\n",
      "323/323 [==============================] - 49s 150ms/step - loss: 0.1850 - accuracy: 0.9406\n",
      "Epoch 34/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1744 - accuracy: 0.9461\n",
      "Epoch 35/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1182 - accuracy: 0.9649\n",
      "Epoch 36/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1108 - accuracy: 0.9651\n",
      "Epoch 37/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1186 - accuracy: 0.9617\n",
      "Epoch 38/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1340 - accuracy: 0.9589\n",
      "Epoch 39/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0916 - accuracy: 0.9724\n",
      "Epoch 40/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1027 - accuracy: 0.9688\n",
      "Epoch 41/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1056 - accuracy: 0.9686\n",
      "Epoch 42/150\n",
      "323/323 [==============================] - 49s 150ms/step - loss: 0.0932 - accuracy: 0.9718\n",
      "Epoch 43/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0906 - accuracy: 0.9723\n",
      "Epoch 44/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0941 - accuracy: 0.9707\n",
      "Epoch 45/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0756 - accuracy: 0.9781\n",
      "Epoch 46/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0943 - accuracy: 0.9697\n",
      "Epoch 47/150\n",
      "323/323 [==============================] - 48s 150ms/step - loss: 0.0779 - accuracy: 0.9754\n",
      "Epoch 48/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0768 - accuracy: 0.9784\n",
      "Epoch 49/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0775 - accuracy: 0.9766\n",
      "Epoch 50/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0802 - accuracy: 0.9763\n",
      "Epoch 51/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0491 - accuracy: 0.9862\n",
      "Epoch 52/150\n",
      "323/323 [==============================] - 49s 150ms/step - loss: 0.0674 - accuracy: 0.9799\n",
      "Epoch 53/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0776 - accuracy: 0.9771\n",
      "Epoch 54/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0669 - accuracy: 0.9793\n",
      "Epoch 55/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0744 - accuracy: 0.9777\n",
      "Epoch 56/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0433 - accuracy: 0.9882\n",
      "Epoch 57/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0634 - accuracy: 0.9806\n",
      "Epoch 58/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0581 - accuracy: 0.9844\n",
      "Epoch 59/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0797 - accuracy: 0.9757\n",
      "Epoch 60/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0658 - accuracy: 0.9806\n",
      "Epoch 61/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0678 - accuracy: 0.9802\n",
      "Epoch 62/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0241 - accuracy: 0.9957\n",
      "Epoch 63/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0464 - accuracy: 0.9869\n",
      "Epoch 64/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0639 - accuracy: 0.9807\n",
      "Epoch 65/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0604 - accuracy: 0.9828\n",
      "Epoch 66/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0593 - accuracy: 0.9828\n",
      "Epoch 67/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0542 - accuracy: 0.9865\n",
      "Epoch 68/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0329 - accuracy: 0.9920\n",
      "Epoch 69/150\n",
      "323/323 [==============================] - 48s 150ms/step - loss: 0.0542 - accuracy: 0.9845\n",
      "Epoch 70/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0497 - accuracy: 0.9848\n",
      "Epoch 71/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0576 - accuracy: 0.9825\n",
      "Epoch 72/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0604 - accuracy: 0.9826\n",
      "Score for fold  1: loss of 0.11172425001859665; accuracy of 96.88092470169067%\n",
      "---------------------------------------------------\n",
      "Training for fold 2 -------\n",
      "Epoch 1/150\n",
      "323/323 [==============================] - 68s 151ms/step - loss: 4.7100 - accuracy: 0.0259\n",
      "Epoch 2/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 3.6159 - accuracy: 0.0314\n",
      "Epoch 3/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 3.5781 - accuracy: 0.0292\n",
      "Epoch 4/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 3.3862 - accuracy: 0.0645\n",
      "Epoch 5/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 3.0313 - accuracy: 0.1233\n",
      "Epoch 6/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.3792 - accuracy: 0.2754\n",
      "Epoch 7/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.9893 - accuracy: 0.3910\n",
      "Epoch 8/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.6872 - accuracy: 0.4842\n",
      "Epoch 9/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 1.4107 - accuracy: 0.5738\n",
      "Epoch 10/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.1244 - accuracy: 0.6615\n",
      "Epoch 11/150\n",
      "323/323 [==============================] - 49s 150ms/step - loss: 0.9496 - accuracy: 0.7171\n",
      "Epoch 12/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.8027 - accuracy: 0.7616\n",
      "Epoch 13/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.6798 - accuracy: 0.8035\n",
      "Epoch 14/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.5925 - accuracy: 0.8256\n",
      "Epoch 15/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.4986 - accuracy: 0.8536\n",
      "Epoch 16/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.4456 - accuracy: 0.8693\n",
      "Epoch 17/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3882 - accuracy: 0.8853\n",
      "Epoch 18/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3371 - accuracy: 0.9011\n",
      "Epoch 19/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.3018 - accuracy: 0.9124\n",
      "Epoch 20/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2695 - accuracy: 0.9231\n",
      "Epoch 21/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2353 - accuracy: 0.9323\n",
      "Epoch 22/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2181 - accuracy: 0.9344\n",
      "Epoch 23/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1961 - accuracy: 0.9412\n",
      "Epoch 24/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1771 - accuracy: 0.9492\n",
      "Epoch 25/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1409 - accuracy: 0.9583\n",
      "Epoch 26/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1403 - accuracy: 0.9584\n",
      "Epoch 27/150\n",
      "323/323 [==============================] - 48s 150ms/step - loss: 0.1335 - accuracy: 0.9598\n",
      "Epoch 28/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1143 - accuracy: 0.9688\n",
      "Epoch 29/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0975 - accuracy: 0.9716\n",
      "Epoch 30/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0973 - accuracy: 0.9746\n",
      "Epoch 31/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0947 - accuracy: 0.9740\n",
      "Epoch 32/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1026 - accuracy: 0.9706\n",
      "Epoch 33/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1003 - accuracy: 0.9708\n",
      "Epoch 34/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0646 - accuracy: 0.9833\n",
      "Epoch 35/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0764 - accuracy: 0.9800\n",
      "Epoch 36/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0709 - accuracy: 0.9805\n",
      "Epoch 37/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0783 - accuracy: 0.9778\n",
      "Epoch 38/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0667 - accuracy: 0.9811\n",
      "Epoch 39/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0862 - accuracy: 0.9746\n",
      "Epoch 40/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.0433 - accuracy: 0.9897\n",
      "Epoch 41/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0498 - accuracy: 0.9872\n",
      "Epoch 42/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0789 - accuracy: 0.9770\n",
      "Epoch 43/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0700 - accuracy: 0.9832\n",
      "Epoch 44/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0488 - accuracy: 0.9885\n",
      "Epoch 45/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0495 - accuracy: 0.9878\n",
      "Epoch 46/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0530 - accuracy: 0.9885\n",
      "Epoch 47/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0580 - accuracy: 0.9851\n",
      "Epoch 48/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0479 - accuracy: 0.9882\n",
      "Epoch 49/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0487 - accuracy: 0.9877\n",
      "Epoch 50/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0501 - accuracy: 0.9872\n",
      "Score for fold  2: loss of 0.10027067363262177; accuracy of 96.94985151290894%\n",
      "---------------------------------------------------\n",
      "Training for fold 3 -------\n",
      "Epoch 1/150\n",
      "323/323 [==============================] - 69s 150ms/step - loss: 4.0603 - accuracy: 0.0474\n",
      "Epoch 2/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 3.3160 - accuracy: 0.0818\n",
      "Epoch 3/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 3.1929 - accuracy: 0.1008\n",
      "Epoch 4/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.7943 - accuracy: 0.1753\n",
      "Epoch 5/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 2.2228 - accuracy: 0.3189\n",
      "Epoch 6/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.8629 - accuracy: 0.4322\n",
      "Epoch 7/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 1.5751 - accuracy: 0.5399\n",
      "Epoch 8/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.3051 - accuracy: 0.6175\n",
      "Epoch 9/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 1.0714 - accuracy: 0.6976\n",
      "Epoch 10/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.9084 - accuracy: 0.7461\n",
      "Epoch 11/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.7541 - accuracy: 0.7867\n",
      "Epoch 12/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.6300 - accuracy: 0.8240\n",
      "Epoch 13/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.5165 - accuracy: 0.8551\n",
      "Epoch 14/150\n",
      "323/323 [==============================] - 49s 153ms/step - loss: 0.4458 - accuracy: 0.8733\n",
      "Epoch 15/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.3605 - accuracy: 0.8999\n",
      "Epoch 16/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.2911 - accuracy: 0.9212\n",
      "Epoch 17/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2562 - accuracy: 0.9291\n",
      "Epoch 18/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.2278 - accuracy: 0.9405\n",
      "Epoch 19/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1893 - accuracy: 0.9501\n",
      "Epoch 20/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1684 - accuracy: 0.9542\n",
      "Epoch 21/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1344 - accuracy: 0.9677\n",
      "Epoch 22/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1292 - accuracy: 0.9657\n",
      "Epoch 23/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1325 - accuracy: 0.9665\n",
      "Epoch 24/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1069 - accuracy: 0.9729\n",
      "Epoch 25/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.1080 - accuracy: 0.9750\n",
      "Epoch 26/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.1124 - accuracy: 0.9720\n",
      "Epoch 27/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0829 - accuracy: 0.9810\n",
      "Epoch 28/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0796 - accuracy: 0.9829\n",
      "Epoch 29/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0923 - accuracy: 0.9781\n",
      "Epoch 30/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0950 - accuracy: 0.9758\n",
      "Epoch 31/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0769 - accuracy: 0.9836\n",
      "Epoch 32/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0763 - accuracy: 0.9833\n",
      "Epoch 33/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0769 - accuracy: 0.9816\n",
      "Epoch 34/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0707 - accuracy: 0.9860\n",
      "Epoch 35/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0782 - accuracy: 0.9823\n",
      "Epoch 36/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0780 - accuracy: 0.9827\n",
      "Epoch 37/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0653 - accuracy: 0.9880\n",
      "Epoch 38/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0646 - accuracy: 0.9858\n",
      "Epoch 39/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0680 - accuracy: 0.9867\n",
      "Epoch 40/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0729 - accuracy: 0.9848\n",
      "Epoch 41/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0657 - accuracy: 0.9866\n",
      "Epoch 42/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0707 - accuracy: 0.9859\n",
      "Epoch 43/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0631 - accuracy: 0.9887\n",
      "Epoch 44/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0675 - accuracy: 0.9877\n",
      "Epoch 45/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0646 - accuracy: 0.9891\n",
      "Epoch 46/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0748 - accuracy: 0.9843\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 49s 150ms/step - loss: 0.0769 - accuracy: 0.9830\n",
      "Epoch 48/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0592 - accuracy: 0.9909\n",
      "Epoch 49/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0732 - accuracy: 0.9856\n",
      "Epoch 50/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0692 - accuracy: 0.9870\n",
      "Epoch 51/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0743 - accuracy: 0.9867\n",
      "Epoch 52/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0721 - accuracy: 0.9879\n",
      "Epoch 53/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0753 - accuracy: 0.9860\n",
      "Epoch 54/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0825 - accuracy: 0.9836\n",
      "Epoch 55/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0812 - accuracy: 0.9849\n",
      "Epoch 56/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0690 - accuracy: 0.9894\n",
      "Epoch 57/150\n",
      "323/323 [==============================] - 49s 152ms/step - loss: 0.0807 - accuracy: 0.9858\n",
      "Epoch 58/150\n",
      "323/323 [==============================] - 49s 151ms/step - loss: 0.0836 - accuracy: 0.9849\n",
      "Score for fold  3: loss of 0.12087604403495789; accuracy of 96.65632247924805%\n",
      "Average Score per fold \n",
      "-----------------------------------------------\n",
      "> Fold 1 - Loss: 0.11172425001859665 - Accuracy: 96.88092470169067%\n",
      "-----------------------------------------------\n",
      "> Fold 2 - Loss: 0.10027067363262177 - Accuracy: 96.94985151290894%\n",
      "-----------------------------------------------\n",
      "> Fold 3 - Loss: 0.12087604403495789 - Accuracy: 96.65632247924805%\n",
      "-----------------------------------------------\n",
      "Average Metrics for all folds: \n",
      "> Accuracy: 96.82903289794922 (+- 0.125324626130515)\n",
      "> Loss: 0.11095698922872543\n",
      "-----------------------------------------------\n",
      "************************************************\n",
      "For subject 20 without noise\n",
      "The loss is 0.11411860585212708 and accuracy is 96.96691036224365%\n",
      "For subject 20 and noise 5 dB\n",
      "The loss is 0.20713895559310913 and accuracy is 95.65716981887817%\n",
      "For subject 20 and noise 10 dB\n",
      "The loss is 0.21393366158008575 and accuracy is 95.54228186607361%\n",
      "For subject 20 and noise 15 dB\n",
      "The loss is 0.21919240057468414 and accuracy is 95.42738795280457%\n",
      "************************************************\n"
     ]
    }
   ],
   "source": [
    "run_experiment(input_size, n_folds,  config.batch_size, config.epochs, start_subject, total_subject, session, path, \n",
    "                       input_type,n_channels, extend, extend_size, center, whiten, ratio, noise_db, std,\n",
    "                       config.projected_dim, config.patch_size, config.num_shift_blocks_per_stages,\n",
    "                        config.epsilon, config.mlp_dropout_rate, config.stochastic_depth_rate, config.num_div, config.shift_pixel,\n",
    "                        config.mlp_expand_ratio, type_of_experiment, config.lr_start, config.lr_max, config.weight_decay)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "shiftvit",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "decomp",
   "language": "python",
   "name": "decomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
